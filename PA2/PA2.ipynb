{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display_markdown\n",
    "import os\n",
    "from IPython.display import Markdown\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from typing import TypedDict\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some free LLM providers you can set up (be mindful of their limits, use delays if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use any of the models you want, **as long you're able to complete the assignment** <br>\n",
    "A word of caution: For tasks involving tool-calling, LLama is terrible. Small mistral/gemini models work fine most of the times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://console.groq.com/keys\n",
    "2. https://ai.google.dev/gemini-api/docs/models#experimental\n",
    "3. https://docs.mistral.ai/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "## Feel free to switch up the models. E.g. Mistral's limits are imposed model-wise, so you switch between small/medium.\n",
    "\n",
    "llm = ChatMistralAI(api_key=\"<>\", model=\"mistral-medium-latest\")\n",
    "\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]= \"<>\"\n",
    "client = Groq()\n",
    "llm = ChatGroq(api_key = \"<>\",model=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gemini-flash-lite-latest\",\n",
    "    api_key=\"<>\",\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 1: The Reflection Pattern with LangGraph**\n",
    "\n",
    "In this section, you'll learn to build agentic workflows using `LangGraph`, a powerful library from LangChain for creating complex, stateful, and potentially cyclical agent runtimes. Think of it as building a flowchart for your agents.\n",
    "\n",
    "We'll start with a very simple \"A to B\" workflow to understand the core concepts, and then you'll apply that knowledge to build a more advanced reflection agent.\n",
    "\n",
    "### **Part 1: A Beginner's Guide to LangGraph**\n",
    "\n",
    "Before we build a looping agent, let's understand the basics with a simple, linear task:\n",
    "1.  **Agent 1 (Poet):** Writes a short poem about the sky.\n",
    "2.  **Agent 2 (Translator):** Translates that poem into French.\n",
    "\n",
    "This will teach you the three core components of any LangGraph workflow:\n",
    "\n",
    "1.  **The State:** A shared object that holds information and is passed between agents.\n",
    "2.  **The Nodes:** The \"workers\" or agents in our graph. Each node is a Python function that performs an action.\n",
    "3.  **The Edges:** The connections that define the path of the workflow, directing the flow from one node to the next.\n",
    "\n",
    "#### **Step 1: Define the Graph State**\n",
    "\n",
    "Our state needs to hold the original poem and the translated version. We use a `TypedDict` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class PoemWorkflowState(TypedDict):\n",
    "    \"\"\"A state that holds the poem and its translation.\"\"\"\n",
    "    poem: str\n",
    "    translated_poem: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 2: Define the Agent Nodes**\n",
    "\n",
    "Each agent is a function that takes the current `state` as input and returns a dictionary with the fields it wants to update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatMistralAI(api_key=\"\", model=\"mistral-large-latest\")\n",
    "\n",
    "def poet_node(state: PoemWorkflowState):\n",
    "    \"\"\"Generates a poem.\"\"\"\n",
    "    print(\"--- âœ’ï¸ POET NODE ---\")\n",
    "    prompt = ChatPromptTemplate.from_template(\"Write the first four (non-identical) stanza of Iqbal's Jawab-e-Shikwa poem in Urdu\")\n",
    "    chain = prompt | llm\n",
    "    poem_result = chain.invoke({})\n",
    "    # Return a dictionary to update the 'poem' field in the state\n",
    "    return {\"poem\": poem_result.content}\n",
    "\n",
    "def translator_node(state: PoemWorkflowState):\n",
    "    \"\"\"Translates the poem in the state.\"\"\"\n",
    "    print(\"--- ðŸŒ TRANSLATOR NODE ---\")\n",
    "    # The 'poem' field was populated by the previous node\n",
    "    poem_to_translate = state[\"poem\"]\n",
    "    prompt = ChatPromptTemplate.from_template(\"Translate the following poem into English:\\n\\n{poem}\")\n",
    "    chain = prompt | llm\n",
    "    translation_result = chain.invoke({\"poem\": poem_to_translate})\n",
    "    # Return a dictionary to update the 'translated_poem' field\n",
    "    return {\"translated_poem\": translation_result.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 3: Wire up the Graph**\n",
    "\n",
    "Now we define the flowchart: start at the `poet_node`, then go to the `translator_node`, and then end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Create a new graph\n",
    "workflow = StateGraph(PoemWorkflowState)\n",
    "\n",
    "# Add the two nodes we defined\n",
    "workflow.add_node(\"poet\", poet_node)\n",
    "workflow.add_node(\"translator\", translator_node)\n",
    "\n",
    "# Set the entry point of the workflow\n",
    "workflow.set_entry_point(\"poet\")\n",
    "\n",
    "# Define the connections (edges)\n",
    "# After the 'poet' node, the workflow should go to the 'translator' node\n",
    "workflow.add_edge(\"poet\", \"translator\")\n",
    "# The 'translator' node is the last step, so we connect it to the special END node\n",
    "workflow.add_edge(\"translator\", END)\n",
    "\n",
    "# Compile the graph into a runnable app\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 4: Run the Workflow**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state = app.invoke({})\n",
    "\n",
    "print(\"\\n--- âœ… WORKFLOW COMPLETE ---\")\n",
    "print(\"\\nOriginal Poem:\")\n",
    "print(final_state['poem'])\n",
    "print(\"\\nTranslated Poem:\")\n",
    "print(final_state['translated_poem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: all these free models are terrible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 2: Your Task - Build a Reflection Agent** [30 marks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you understand the basics of `State`, `Nodes`, and `Edges`, you will build the more complex reflection agent. This agent will have a **cyclical** workflow: **Generate -> Reflect -> (Decide) -> Generate...**\n",
    "\n",
    "**Goal:** Create a workflow that writes a Python script to scrape Hacker News, and then iteratively refines it based on expert critique.\n",
    "\n",
    "Follow the `TODO` comments below to implement the full graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://console.groq.com/keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.langchain.com/oss/python/langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's save mistral credits for later\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]= \"your_groq_api_key_here\"\n",
    "client = Groq()\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`TODO 1`: Define the Graph State** [5 marks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    # TODO: Define the fields for the graph state.\n",
    "    # You will need:\n",
    "    # task: str - The user's initial request\n",
    "    # code: str - The Python code generated by the agent\n",
    "    # critiques: List[str] - A list of critiques from the reflection agent\n",
    "    # revisions: int - A counter for how many revisions have been made\n",
    "    pass # Remove this line after defining the fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`TODO 2 & 3`: Implement the Agent Nodes** [10 marks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GENERATION NODE ---\n",
    "def generation_node(state: GraphState):\n",
    "    \"\"\"Generates the code based on the current state.\"\"\"\n",
    "    print(\"--- ðŸ’» GENERATING CODE ---\")\n",
    "    # TODO: Implement the generation logic.\n",
    "    # 1. Get the task, critiques, and revisions from the state dictionary.\n",
    "    # 2. Check if revisions == 0.\n",
    "    #    - If it is, create a prompt to generate the initial code.\n",
    "    #    - If it's not, create a different prompt that asks the agent to revise the code based on the critiques.\n",
    "    # 3. Create a chain (prompt | llm) and invoke it.\n",
    "    # 4. Return a dictionary to update the 'code' and 'revisions' fields in the state.\n",
    "    pass # Remove this line\n",
    "\n",
    "# --- REFLECTION NODE ---\n",
    "def reflection_node(state: GraphState):\n",
    "    \"\"\"Reflects on the code and provides critiques.\"\"\"\n",
    "    print(\"--- ðŸ¤” REFLECTING ON CODE ---\")\n",
    "    # TODO: Implement the reflection logic.\n",
    "    # 1. Get the 'code' from the state dictionary.\n",
    "    # 2. Create a system prompt for a Senior Python Developer who is reviewing the code.\n",
    "    # 3. Create a user message containing the code to be reviewed. Remember to use a template variable like {code}.\n",
    "    # 4. Create a chain and invoke it, passing the code as input.\n",
    "    # 5. Return a dictionary to update the 'critiques' field in the state.\n",
    "    pass # Remove this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`TODO 4`: Implement the Conditional Edge** [5 marks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "\n",
    "def should_continue(state: GraphState):\n",
    "    \"\"\"Determines whether to continue the reflection loop.\"\"\"\n",
    "    # TODO: Implement the conditional logic.\n",
    "    # 1. Get the 'revisions' count from the state.\n",
    "    # 2. If the number of revisions is 2 or more, print a message and return \"end\".\n",
    "    # 3. Otherwise, print a message and return \"continue\".\n",
    "    pass # Remove this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`TODO 5`: Wire Up the Graph** [10 marks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.langchain.com/oss/python/langgraph/graph-api#conditional-edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# TODO: Add the nodes, entry point, and edges to the workflow.\n",
    "# 1. Add the \"generator\" and \"reflector\" nodes.\n",
    "# 2. Set the entry point to be the \"generator\".\n",
    "# 3. Add a standard edge from the \"generator\" to the \"reflector\".\n",
    "# 4. Add a CONDITIONAL edge from the \"reflector\" node.\n",
    "#    - This edge should call your 'should_continue' function.\n",
    "#    - If the function returns \"continue\", the graph should go back to the \"generator\" node.\n",
    "#    - If the function returns \"end\", the graph should go to the END node.\n",
    "\n",
    "# Compile the graph into a runnable app\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"Create a Python function using requests and BeautifulSoup that scrapes the titles of the top 5 articles from the Hacker News homepage (https://news.ycombinator.com).\"\n",
    "initial_input = {\"task\": task} # The initial state only needs the task\n",
    "\n",
    "final_state = app.invoke(initial_input)\n",
    "\n",
    "print(\"\\n--- âœ¨ FINAL, REFINED CODE ---\")\n",
    "display(Markdown(final_state['code']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 2: Tool Calling with LangChain**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An LLM's knowledge is frozen in time and it has no access to the outside world. To build truly powerful applications, we need to give our agents **tools**â€”functions they can call to interact with APIs, databases, or any other external system.\n",
    "\n",
    "LangChain provides a seamless way to equip agents with tools and let them decide when to use them.\n",
    "\n",
    "### **Part 1: A Beginner's Guide to Tool Calling**\n",
    "\n",
    "Let's start with a very simple example: giving an agent a calculator.\n",
    "\n",
    "This will teach you the three key components of a LangChain tool-calling agent:\n",
    "1.  **The Tool:** A Python function decorated with `@tool`.\n",
    "2.  **The Agent:** The \"brain\" that decides which tool to use. We'll use `create_tool_calling_agent`.\n",
    "3.  **The AgentExecutor:** The runtime that actually executes the tool calls and passes the results back to the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 1: Define a Tool**\n",
    "\n",
    "Any Python function can become a tool. The magic is in the `@tool` decorator, which automatically converts the function's signature and docstring into a format the LLM can understand.\n",
    "\n",
    "> **Important:** A clear, descriptive docstring is crucial. The agent uses the docstring to figure out *what the tool does* and *when to use it*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies two integers together.\"\"\"\n",
    "    print(\"Bro needs to multiply\", a, \"and\", b)\n",
    "    return a * b\n",
    "\n",
    "# We create a list of all the tools the agent will have access to.\n",
    "tools = [multiply]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 2: Create a Tool-Calling Agent and Executor**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we assemble the agent. We need the LLM, our list of tools, and a special prompt.\n",
    "\n",
    "The prompt is the agent's instruction manual. We'll use a pre-built template from LangChain which includes a special placeholder: `agent_scratchpad`. This is where the agent will keep track of its internal thoughts and previous tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\") \n",
    "\n",
    "# The agent is created with the model, tools, and a system prompt.\n",
    "# This single object is now the complete, runnable agent.\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    system_prompt=\"You are a helpful assistant. You must use your tools to answer questions.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 3: Run the Agent**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask a question that requires the agent to use its `multiply` tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro needs to multiply 8 and 7\n",
      "The result of 8 times 7 is 56.\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke({\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is 8 times 7?\"}\n",
    "    ]\n",
    "})\n",
    "\n",
    "# The final answer is in the 'content' of the last message in the output.\n",
    "final_answer = result[\"messages\"][-1].content\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 2: Your Task - Build a Multi-Tool Travel Agent [10 marks]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use the modern `create_agent` function to build a \"Travel Agent\" that can use **multiple tools** to answer a complex user query.\n",
    "\n",
    "**Goal:** Create a single agent that can help a user plan a trip by providing information on flights, weather, and local events.\n",
    "\n",
    "Follow the `TODO` comments below to implement the full agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`TODO 1`: Create the Tools [5 marks]**\n",
    "Define three distinct Python functions. Since we don't have real APIs for this, you will create **mock functions** that return hardcoded string data. Each function must have a clear docstring explaining what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# TODO: Define and decorate three mock tools.\n",
    "# 1. get_flight_info(origin: str, destination: str, month: str) -> str\n",
    "#    - Docstring: \"Provides fictional flight prices and availability for a trip.\"\n",
    "#    - Returns a JSON string with flight details.\n",
    "@tool\n",
    "def get_flight_info(origin: str, destination: str, month: str) -> str:\n",
    "    \"\"\"Provides fictional flight prices and availability for a trip.\"\"\"\n",
    "    pass # Remove this line and implement the function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. get_weather_forecast(city: str, month: str) -> str\n",
    "#    - Docstring: \"Provides a fictional weather forecast for a specific city and month.\"\n",
    "#    - Returns a JSON string with weather details.\n",
    "@tool\n",
    "def get_weather_forecast(city: str, month: str) -> str:\n",
    "    \"\"\"Provides a fictional weather forecast for a specific city and month.\"\"\"\n",
    "    pass # Remove this line and implement the function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. search_city_events(city: str, month: str) -> str\n",
    "#    - Docstring: \"Provides a list of major fictional events for a specific city and month.\"\n",
    "#    - Returns a JSON string with event details.\n",
    "@tool\n",
    "def search_city_events(city: str, month: str) -> str:\n",
    "    \"\"\"Provides a list of major fictional events for a specific city and month.\"\"\"\n",
    "    pass # Remove this line and implement the function\n",
    "\n",
    "\n",
    "# TODO: Create a list called `travel_tools` that contains all three decorated tool objects.\n",
    "travel_tools = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`TODO 2`: Create and Run the Agent ðŸš€ [5 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# TODO: Assemble and run the agent.\n",
    "# 1. Initialize the LLM \n",
    "llm = None # Replace this\n",
    "\n",
    "# 2. Define a clear and concise system prompt for your travel agent.\n",
    "system_prompt = \"\" # Replace this\n",
    "\n",
    "# 3. Create the agent using create_agent, passing the llm, tools list, and system prompt.\n",
    "agent = None # Replace this\n",
    "\n",
    "user_message_content = \"Help me plan a trip to Tel Aviv from Tehran for this June. I need to know about flights, weather, and any major events.\"\n",
    "\n",
    "# 5. Invoke the agent with the correct message format and print the final answer.\n",
    "result = None # Replace this\n",
    "final_answer = result[\"messages\"][-1].content\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 3: Advanced Multi-Agent Collaboration with LangGraph**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now mastered the core patterns of agentic design: stateful workflows with `LangGraph`, tool use with `create_agent`, and multi-agent collaboration. This final project will challenge you to combine all these skills to build a sophisticated, practical research crew.\n",
    "\n",
    "Your goal is to create a multi-agent system that can verify a claim by consulting multiple sources, cross-referencing their findings, and looping its research until it reaches a confident conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using the Tavily Search Tool**\n",
    "For this task, we will use the **Tavily Search API** for live web searches. It is a powerful tool designed specifically for LLM agents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tavily.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# It is recommended to use a secrets manager for your keys.\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, you're free to use any model here on. But to warn you, several models (e.g. LLama we used earlier) are absolutely terrible at calling tools appropriately. Mistral should work fine, most of the times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "from langchain.agents import create_agent\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# You can configure the tool's parameters, like max_results, upon instantiation\n",
    "search_tool = TavilySearch(max_results=3)\n",
    "\n",
    "\n",
    "# You can then pass this tool in a list to your agent like this and it will automatically perform a search if needed\n",
    "llm = ChatMistralAI(api_key=\"\", model=\"mistral-small-latest\")\n",
    "my_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[search_tool], # Pass the tool here\n",
    "    system_prompt=\"You are a web researcher.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Your Task: Build the Fact-Checking Crew [30 marks]**\n",
    "\n",
    "#### **High-Level Architecture**\n",
    "\n",
    "Your crew will now include a skeptical agent to ensure a balanced perspective.\n",
    "1.  **Start with a claim.**\n",
    "2.  **Initial Research:** A news-focused agent will search for evidence supporting the claim.\n",
    "3.  **Adversarial Research:** A \"Devil's Advocate\" agent will actively search for evidence that *contradicts* the claim.\n",
    "4.  **Verification:** A lead verifier will analyze both the supporting and contradictory evidence, produce a consolidated analysis, and decide on a verdict: `CONFIRMED`, `CONTRADICTED`, or `NEEDS_MORE_INFO`.\n",
    "5.  **Conditional Loop:** If the verdict is `NEEDS_MORE_INFO`, the graph loops back for another round of research.\n",
    "6.  **Final Report:** A writer agent takes the final, verified analysis and produces a polished report.\n",
    "\n",
    "#### **`TODO 1`: Define the Graph State [5 marks]**\n",
    "The state needs to track the claim, the findings from the pro and con agents, the verifier's analysis, and the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "\n",
    "class FactCheckCrewState(TypedDict):\n",
    "    # TODO: Define the fields for the graph state.\n",
    "    # claim: - The initial user claim to be verified.\n",
    "    # revision_number: - A counter for the number of research loops.\n",
    "    # supporting_evidence: - The output from the news search agent.\n",
    "    # counter_evidence:  - The output from the Devil's Advocate agent.\n",
    "    # verified_analysis:  - The consolidated analysis from the lead verifier.\n",
    "    # final_verdict:  - The final verdict (\"CONFIRMED\", \"CONTRADICTED\", etc.).\n",
    "    # final_report:  - The final, polished report from the writer.\n",
    "    pass # Remove this line after defining the fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`TODO 2`: Define the Agent Nodes [15 marks]**\n",
    "You will now create four distinct agent nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_tavily import TavilySearchResults\n",
    "\n",
    "#Use this search tool to enable live searches\n",
    "search_tool = TavilySearch(max_results=4)\n",
    "\n",
    "llm = ...\n",
    "\n",
    "# --- SUPPORTING EVIDENCE AGENT ---\n",
    "def supporting_evidence_node(state: FactCheckCrewState):\n",
    "    \"\"\"Agent 1: Uses the search tool to find recent news and articles that SUPPORT the claim.\"\"\"\n",
    "    print(\"--- ðŸ”Ž SUPPORTING EVIDENCE NODE ---\")\n",
    "    # TODO: Implement this node using create_agent.\n",
    "    # 1. Give it the `search_tool`.\n",
    "    # 2. The system prompt should instruct it to act as a research assistant, finding credible news, studies, and expert opinions that support the claim.\n",
    "    # 3. Invoke the agent with the claim from the state.\n",
    "    # 4. Return a dictionary to update the 'supporting_evidence'.\n",
    "    pass # Remove this line\n",
    "\n",
    "# --- DEVIL'S ADVOCATE AGENT ---\n",
    "def devils_advocate_node(state: FactCheckCrewState):\n",
    "    \"\"\"Agent 2: Uses the search tool to find evidence that CONTRADICTS the claim.\"\"\"\n",
    "    print(\"--- ðŸ˜ˆ DEVIL'S ADVOCATE NODE ---\")\n",
    "    # TODO: Implement this node using create_agent.\n",
    "    # 1. Give it the `search_tool`.\n",
    "    # 2. The system prompt is key: instruct it to be a skeptical \"Devil's Advocate\". Its sole purpose is to find counter-arguments, dissenting opinions, and evidence that debunks or challenges the claim.\n",
    "    # 3. Invoke the agent with the claim from the state.\n",
    "    # 4. Return a dictionary to update the 'counter_evidence'.\n",
    "    pass # Remove this line\n",
    "\n",
    "# --- LEAD VERIFIER AGENT ---\n",
    "def lead_verifier_node(state: FactCheckCrewState):\n",
    "    \"\"\"Agent 3: Synthesizes both sides and makes a verdict. Does not use tools.\"\"\"\n",
    "    print(\"--- âš–ï¸ LEAD VERIFIER NODE ---\")\n",
    "    # TODO: Implement this node.\n",
    "    # 1. Create a detailed prompt that receives the 'supporting_evidence' and 'counter_evidence'.\n",
    "    # 2. Instruct the LLM to act as a neutral judge. It must weigh both sets of evidence, highlight the key arguments from each side, and form a balanced conclusion.\n",
    "    # 3. CRUCIAL: The prompt MUST instruct the agent to output a JSON string with two keys:\n",
    "    #    - \"analysis\": (string) A summary of the verified findings.\n",
    "    #    - \"verdict\": (string) One of three exact values: \"CONFIRMED\", \"CONTRADICTED\", or \"NEEDS_MORE_INFO\".\n",
    "    # 4. Parse the JSON output and return a dictionary to update 'verified_analysis', 'final_verdict', and increment 'revision_number'.\n",
    "    pass # Remove this line\n",
    "\n",
    "# --- REPORT WRITER AGENT ---\n",
    "def report_writer_node(state: FactCheckCrewState):\n",
    "    \"\"\"Agent 4: Writes the final, polished report.\"\"\"\n",
    "    print(\"--- ðŸ–‹ï¸ REPORT WRITER NODE ---\")\n",
    "    # TODO: Implement this node.\n",
    "    # 1. Create a prompt that takes the 'verified_analysis' and the 'final_verdict'.\n",
    "    # 2. Instruct the LLM to write a clear, neutral, and well-structured report for a general audience.\n",
    "    # 3. Return a dictionary to update the 'final_report'.\n",
    "    pass # Remove this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`TODO 3`: Implement the Conditional Edge [5 marks]**\n",
    "This function will read the `final_verdict` and decide the next step for the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue_verification(state: FactCheckCrewState):\n",
    "    \"\"\"Determines whether to loop back for more research or finish.\"\"\"\n",
    "    print(\"--- ðŸ” DECISION NODE ---\")\n",
    "    # TODO: Implement the conditional logic.\n",
    "    # 1. Get the 'final_verdict' and 'revision_number' from the state.\n",
    "    # 2. If the verdict is \"NEEDS_MORE_INFO\" AND the revision number is less than 2, return \"continue_research\".\n",
    "    # 3. Otherwise, return \"finish_report\".\n",
    "    pass # Remove this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`TODO 4`: Wire Up the Graph [5 marks]**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# TODO: Build the graph.\n",
    "# 1. Instantiate StateGraph with your FactCheckCrewState.\n",
    "# 2. Add all four of your nodes.\n",
    "# 3. Define the workflow:\n",
    "#    - The entry point is \"supporting_evidence\".\n",
    "#    - supporting_evidence -> devils_advocate -> lead_verifier\n",
    "#    - After \"lead_verifier\", add the CONDITIONAL edge.\n",
    "#      - \"continue_research\" path should loop back to \"supporting_evidence\".\n",
    "#      - \"finish_report\" path should go to \"report_writer\".\n",
    "#    - \"report_writer\" is the final step before the END.\n",
    "# 4. Compile the graph.\n",
    "\n",
    "app = None # Replace this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Run Your Completed Crew**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim = \"Pakistan gained independence in 1947.\"\n",
    "initial_input = {\"claim\": claim, \"revision_number\": 0}\n",
    "\n",
    "final_state = app.invoke(initial_input)\n",
    "\n",
    "print(\"\\n\\n--- âœ… FINAL REPORT ---\")\n",
    "print(final_state['final_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 4: A Comparative Study of Fact-Checking Agents** [20 marks]\n",
    "\n",
    "How much better is a complex agent than a simple one? You will answer that question by empirically evaluating three different fact-checking methods against a real-world dataset of claims.\n",
    "\n",
    "#### **The Goal**\n",
    "\n",
    "You will take a dataset of fact-checked claims and run each claim through three different verifiers:\n",
    "1.  **Method 1: The Zero-Shot LLM:** A baseline agent with no tools, relying solely on its internal knowledge.\n",
    "2.  **Method 2: The Simple Search Agent:** A single agent equipped with a web search tool (a basic RAG approach).\n",
    "3.  **Method 3: The Advanced Research Crew:** The multi-agent, adversarial fact-checking crew you just built.\n",
    "\n",
    "Finally, you will compare the accuracy of each method to determine the value of agentic complexity.\n",
    "\n",
    "#### **Setup: Loading the Dataset**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('claims.csv')\n",
    "    claims_sample = df[:10].copy()\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'claims.csv' not found.\")\n",
    "    claims_sample = pd.DataFrame()\n",
    "\n",
    "def normalize_verdict(verdict):\n",
    "    if verdict:\n",
    "        return \"true\"\n",
    "    else:\n",
    "        return \"false\"\n",
    "    \n",
    "\n",
    "if not claims_sample.empty:\n",
    "    claims_sample['ground_truth'] = claims_sample['text review'].apply(normalize_verdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You might want to enforce structured outputs to ensure your final answer is a True/False: <br> \n",
    "https://docs.langchain.com/oss/python/langchain/structured-output <br>\n",
    "https://forum.langchain.com/t/make-a-llm-with-structured-output-call-a-tool/622"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 1: Baseline Fact-Checker (Zero-Shot LLM) [5 marks]**\n",
    "\n",
    "This agent has no access to the outside world. It will make its judgment based only on the information it was trained on.\n",
    "\n",
    "**Your Task:** Implement the `verify_claim_zero_shot` function. This function should use a simple LLM chain to classify a claim as \"True\" or \"False\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All our necessary imports from previous sections\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "llm = ...\n",
    "search_tool = TavilySearch(max_results=4)\n",
    "\n",
    "def verify_claim_zero_shot(claim: str) -> str:\n",
    "    \"\"\"Verifies a claim using only the LLM's internal knowledge.\"\"\"\n",
    "    # TODO: Implement the zero-shot verifier.\n",
    "    # 1. Create a ChatPromptTemplate. The system prompt should instruct the LLM\n",
    "    #    to act as a fact-checker and verify the claim.\n",
    "    # 2. CRUCIAL: The prompt MUST instruct the model to respond with ONLY the word \"True\" or \"False\".\n",
    "    # 3. Create a simple chain (prompt | llm).\n",
    "    # 4. Invoke the chain with the claim.\n",
    "    # 5. Extract the text content from the result.\n",
    "    # 6. Add a small cleanup step: if \"true\" is in the lowercased result, return \"True\". Otherwise, return \"False\".\n",
    "    #    This makes your function robust to small variations in the LLM's output.\n",
    "    pass # Remove this line\n",
    "\n",
    "# --- Run the evaluation ---\n",
    "print(\"Running Zero-Shot Verifier...\")\n",
    "results_zero_shot = []\n",
    "for claim in tqdm(claims_sample['claim'], desc=\"Zero-Shot Verification\"):\n",
    "    verdict = verify_claim_zero_shot(claim)\n",
    "    results_zero_shot.append(verdict)\n",
    "\n",
    "claims_sample['zero_shot_verdict'] = results_zero_shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 2: Simple RAG Fact-Checker (Agent with Search Tool) [5 marks]**\n",
    "This agent represents a standard RAG (Retrieval-Augmented Generation) approach. It can search the web for information before making a decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Task:** Implement the `verify_claim_with_search` function. This function will create a simple agent equipped with the `TavilySearch` tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_claim_with_search(claim: str) -> str:\n",
    "    \"\"\"Verifies a claim using a single agent with a web search tool.\"\"\"\n",
    "    # TODO: Implement the simple search agent.\n",
    "    # 1. Create an agent using create_agent.\n",
    "    # 2. Provide it with a list containing just one tool: `search_tool`.\n",
    "    # 3. The system prompt should instruct it to use its search tool to find information\n",
    "    #    and then make a final judgment on the claim.\n",
    "    # 4. CRUCIAL: The prompt MUST also instruct the model to end its final response\n",
    "    #    with ONLY the word \"True\" or \"False\".\n",
    "    # 5. Invoke the agent with the claim. Enforcing a structured output will help.\n",
    "    # 6. Extract the text content from the final message.\n",
    "    # 7. Use the same cleanup logic as before to return a clean \"True\" or \"False\".\n",
    "    pass # Remove this line\n",
    "\n",
    "# --- Run the evaluation ---\n",
    "print(\"\\nRunning Simple Search Verifier...\")\n",
    "results_with_search = []\n",
    "for claim in tqdm(claims_sample['claim'], desc=\"Search Verification\"):\n",
    "    verdict = verify_claim_with_search(claim)\n",
    "    results_with_search.append(verdict)\n",
    "\n",
    "claims_sample['simple_search_verdict'] = results_with_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 3: Advanced Fact-Checker (Multi-Agent Crew) [5 marks]**\n",
    "\n",
    "**You may re-use the multi-agent crew you built in the previous section.**\n",
    "\n",
    "**Your Task:** Implement the `verify_claim_with_crew` function. This function will invoke your crew and, most importantly, translate its complex output (`CONFIRMED`, `CONTRADICTED`, `NEEDS_MORE_INFO`) into the simple \"True\"/\"False\" format required for evaluation.\n",
    "\n",
    "Note: Restrict the number of revisions to 2. If the crew is not able to arrive at a definitive answer, output 'uncertain'.  I'll leave it upto you to think and decide if such a behavior should be penalized. You may compute your accuracy accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, bring over your complete, working Multi-Agent Crew graph from the previous section.\n",
    "# (The full solution code for the \"Devil's Advocate\" crew is assumed to be here)\n",
    "# ... app = workflow.compile() ...\n",
    "\n",
    "def verify_claim_with_crew(claim: str) -> str:\n",
    "    \"\"\"Verifies a claim using the full multi-agent research crew.\"\"\"\n",
    "    # TODO: Implement the crew-based verifier.\n",
    "    # 1. Define the initial state for your LangGraph app. It needs the 'claim' and a 'revision_number' of 0.\n",
    "    # 2. Invoke the app with this initial state.\n",
    "    # 3. Get the 'final_verdict' from the resulting state dictionary.\n",
    "    # 4. Implement the translation logic:\n",
    "    #    - If the final_verdict is \"CONFIRMED\", return \"True\".\n",
    "    #    - If the final_verdict is \"CONTRADICTED\", return \"False\".\n",
    "    #    - If the final_verdict is \"NEEDS_MORE_INFO\" return \"Other\"\n",
    "    #      (as the claim could not be confidently confirmed).\n",
    "    pass # Remove this line\n",
    "\n",
    "# --- Run the evaluation ---\n",
    "print(\"\\nRunning Multi-Agent Crew Verifier...\")\n",
    "results_with_crew = []\n",
    "# Ensure your crew's LangGraph `app` is defined and compiled in a cell above this one!\n",
    "for claim in tqdm(claims_sample['claim'], desc=\"Crew Verification\"):\n",
    "    verdict = verify_claim_with_crew(claim)\n",
    "    results_with_crew.append(verdict)\n",
    "\n",
    "claims_sample['crew_verdict'] = results_with_crew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Final Evaluation & Analysis [5 marks]**\n",
    "\n",
    "Now for the moment of truth. We will calculate the accuracy of each method by comparing its verdicts to the ground truth from the dataset.\n",
    "\n",
    "**Your Task:** Run the evaluation code and then, in the final markdown cell, write a brief analysis of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate Accuracy ---\n",
    "ground_truth = claims_sample['ground_truth'].values\n",
    "\n",
    "accuracy_zero_shot = (claims_sample['zero_shot_verdict'].values == ground_truth).mean()\n",
    "accuracy_simple_search = (claims_sample['simple_search_verdict'].values == ground_truth).mean()\n",
    "accuracy_crew = (claims_sample['crew_verdict'].values == ground_truth).mean()\n",
    "\n",
    "print(\"--- Final Results ---\")\n",
    "print(f\"Zero-Shot LLM Accuracy: {accuracy_zero_shot:.2%}\")\n",
    "print(f\"Simple Search Agent Accuracy: {accuracy_simple_search:.2%}\")\n",
    "print(f\"Multi-Agent Crew Accuracy: {accuracy_crew:.2%}\")\n",
    "\n",
    "print(\"\\n--- Detailed Comparison ---\")\n",
    "display(claims_sample[['claim', 'ground_truth', 'zero_shot_verdict', 'simple_search_verdict', 'crew_verdict']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Analysis of Results**\n",
    "\n",
    "**(TODO: Write your analysis here in this markdown cell.)**\n",
    "\n",
    "1. **Which method performed the best? Why?**\n",
    "2. **Do you think these results were expected?**\n",
    "3. **If the multi-agent crew was not able to output a final answer - what's better from a social welfare perspective: an output you are not confident about or refraining from giving an output if you are not confident?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
