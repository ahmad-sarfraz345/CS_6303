{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display_markdown\n",
    "import os\n",
    "from IPython.display import Markdown\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from typing import TypedDict\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some free LLM providers you can set up (be mindful of their limits, use delays if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use any of the models you want, **as long you're able to complete the assignment** <br>\n",
    "A word of caution: For tasks involving tool-calling, LLama is terrible. Small mistral/gemini models work fine most of the times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://console.groq.com/keys\n",
    "2. https://ai.google.dev/gemini-api/docs/models#experimental\n",
    "3. https://docs.mistral.ai/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "## Feel free to switch up the models. E.g. Mistral's limits are imposed model-wise, so you switch between small/medium.\n",
    "\n",
    "llm = ChatMistralAI(api_key=os.getenv(\"MISTRAL_API_KEY\"), model=\"mistral-small-latest\")\n",
    "\n",
    "# os.environ[\"GROQ_API_KEY\"]= os.getenv(\"GROQ_API_KEY\")\n",
    "# client = Groq()\n",
    "# llm = ChatGroq(api_key = os.getenv(\"GROQ_API_KEY\"),model=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model=\"gemini-flash-lite-latest\",\n",
    "#     api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "#     base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 1: The Reflection Pattern with LangGraph**\n",
    "\n",
    "In this section, you'll learn to build agentic workflows using `LangGraph`, a powerful library from LangChain for creating complex, stateful, and potentially cyclical agent runtimes. Think of it as building a flowchart for your agents.\n",
    "\n",
    "We'll start with a very simple \"A to B\" workflow to understand the core concepts, and then you'll apply that knowledge to build a more advanced reflection agent.\n",
    "\n",
    "### **Part 1: A Beginner's Guide to LangGraph**\n",
    "\n",
    "Before we build a looping agent, let's understand the basics with a simple, linear task:\n",
    "1.  **Agent 1 (Poet):** Writes a short poem about the sky.\n",
    "2.  **Agent 2 (Translator):** Translates that poem into French.\n",
    "\n",
    "This will teach you the three core components of any LangGraph workflow:\n",
    "\n",
    "1.  **The State:** A shared object that holds information and is passed between agents.\n",
    "2.  **The Nodes:** The \"workers\" or agents in our graph. Each node is a Python function that performs an action.\n",
    "3.  **The Edges:** The connections that define the path of the workflow, directing the flow from one node to the next.\n",
    "\n",
    "#### **Step 1: Define the Graph State**\n",
    "\n",
    "Our state needs to hold the original poem and the translated version. We use a `TypedDict` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class PoemWorkflowState(TypedDict):\n",
    "    \"\"\"A state that holds the poem and its translation.\"\"\"\n",
    "    poem: str\n",
    "    translated_poem: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 2: Define the Agent Nodes**\n",
    "\n",
    "Each agent is a function that takes the current `state` as input and returns a dictionary with the fields it wants to update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poet_node(state: PoemWorkflowState):\n",
    "    \"\"\"Generates a poem.\"\"\"\n",
    "    print(\"--- ‚úíÔ∏è POET NODE ---\")\n",
    "    prompt = ChatPromptTemplate.from_template(\"Write the first four (non-identical) stanza of Iqbal's Jawab-e-Shikwa poem in Urdu\")\n",
    "    chain = prompt | llm\n",
    "    poem_result = chain.invoke({})\n",
    "    # Return a dictionary to update the 'poem' field in the state\n",
    "    return {\"poem\": poem_result.content}\n",
    "\n",
    "def translator_node(state: PoemWorkflowState):\n",
    "    \"\"\"Translates the poem in the state.\"\"\"\n",
    "    print(\"--- üåê TRANSLATOR NODE ---\")\n",
    "    # The 'poem' field was populated by the previous node\n",
    "    poem_to_translate = state[\"poem\"]\n",
    "    prompt = ChatPromptTemplate.from_template(\"Translate the following poem into English:\\n\\n{poem}\")\n",
    "    chain = prompt | llm\n",
    "    translation_result = chain.invoke({\"poem\": poem_to_translate})\n",
    "    # Return a dictionary to update the 'translated_poem' field\n",
    "    return {\"translated_poem\": translation_result.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 3: Wire up the Graph**\n",
    "\n",
    "Now we define the flowchart: start at the `poet_node`, then go to the `translator_node`, and then end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Create a new graph\n",
    "workflow = StateGraph(PoemWorkflowState)\n",
    "\n",
    "# Add the two nodes we defined\n",
    "workflow.add_node(\"poet\", poet_node)\n",
    "workflow.add_node(\"translator\", translator_node)\n",
    "\n",
    "# Set the entry point of the workflow\n",
    "workflow.set_entry_point(\"poet\")\n",
    "\n",
    "# Define the connections (edges)\n",
    "# After the 'poet' node, the workflow should go to the 'translator' node\n",
    "workflow.add_edge(\"poet\", \"translator\")\n",
    "# The 'translator' node is the last step, so we connect it to the special END node\n",
    "workflow.add_edge(\"translator\", END)\n",
    "\n",
    "# Compile the graph into a runnable app\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 4: Run the Workflow**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state = app.invoke({})\n",
    "\n",
    "print(\"\\n--- ‚úÖ WORKFLOW COMPLETE ---\")\n",
    "print(\"\\nOriginal Poem:\")\n",
    "print(final_state['poem'])\n",
    "print(\"\\nTranslated Poem:\")\n",
    "print(final_state['translated_poem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: all these free models are terrible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 2: Your Task - Build a Reflection Agent** [30 marks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you understand the basics of `State`, `Nodes`, and `Edges`, you will build the more complex reflection agent. This agent will have a **cyclical** workflow: **Generate -> Reflect -> (Decide) -> Generate...**\n",
    "\n",
    "**Goal:** Create a workflow that writes a Python script to scrape Hacker News, and then iteratively refines it based on expert critique.\n",
    "\n",
    "Follow the `TODO` comments below to implement the full graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://console.groq.com/keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.langchain.com/oss/python/langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's save mistral credits for later\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]= \"your_groq_api_key_here\"\n",
    "client = Groq()\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`TODO 1`: Define the Graph State** [5 marks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    task: str\n",
    "    code: str\n",
    "    critiques: List[str]\n",
    "    revisions: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`TODO 2 & 3`: Implement the Agent Nodes** [10 marks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GENERATION NODE ---\n",
    "def generation_node(state: GraphState):\n",
    "    \"\"\"Generates the code based on the current state.\"\"\"\n",
    "    print(\"--- üíª GENERATING CODE ---\")\n",
    "    task = state[\"task\"]\n",
    "    critiques = state.get(\"critiques\", [])\n",
    "    revisions = state.get(\"revisions\", 0)\n",
    "    if revisions == 0:\n",
    "        prompt = ChatPromptTemplate.from_template(\"Write Python code to accomplish the following task:\\n\\n{task}\")\n",
    "        chain = prompt | llm\n",
    "        code_result = chain.invoke({\"task\": task})\n",
    "        return {\"code\": code_result.content, \"revisions\": revisions + 1}\n",
    "    else:\n",
    "        critiques_text = '\\n'.join(critiques)\n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            \"Revise the following Python code based on these critiques:\\n\\n\"\n",
    "            \"Code:\\n{code}\\n\\n\"\n",
    "            \"Critiques:\\n{critiques}\"\n",
    "        )\n",
    "        chain = prompt | llm\n",
    "        code_result = chain.invoke({\"code\": state[\"code\"], \"critiques\": critiques_text})\n",
    "        return {\"code\": code_result.content, \"revisions\": revisions + 1}\n",
    "\n",
    "# --- REFLECTION NODE ---\n",
    "def reflection_node(state: GraphState):\n",
    "    \"\"\"Reflects on the code and provides critiques.\"\"\"\n",
    "    print(\"--- ü§î REFLECTING ON CODE ---\")\n",
    "    code_to_review = state[\"code\"]\n",
    "    critiques = state.get(\"critiques\", [])\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"You are a Senior Python Developer. Review the following code and provide constructive critiques:\\n\\n{code}\"\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    review_result = chain.invoke({\"code\": code_to_review})\n",
    "    return {\"critiques\": critiques + [review_result.content]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`TODO 4`: Implement the Conditional Edge** [5 marks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "\n",
    "def should_continue(state: GraphState):\n",
    "    \"\"\"Determines whether to continue the reflection loop.\"\"\"\n",
    "    revisions = state[\"revisions\"]\n",
    "    if revisions >= 2:\n",
    "        print(\"Reached maximum revisions. Ending workflow.\")\n",
    "        return \"end\"\n",
    "    else:\n",
    "        print(\"Revisions remaining. Continuing workflow.\")\n",
    "        return \"continue\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`TODO 5`: Wire Up the Graph** [10 marks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.langchain.com/oss/python/langgraph/graph-api#conditional-edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"generator\", generation_node)\n",
    "workflow.add_node(\"reflector\", reflection_node)\n",
    "workflow.set_entry_point(\"generator\")\n",
    "workflow.add_edge(\"generator\", \"reflector\")\n",
    "workflow.add_conditional_edges(\"reflector\", should_continue, {\"continue\": \"generator\", \"end\": END})\n",
    "\n",
    "# Compile the graph into a runnable app\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üíª GENERATING CODE ---\n",
      "--- ü§î REFLECTING ON CODE ---\n",
      "--- ü§î REFLECTING ON CODE ---\n",
      "Revisions remaining. Continuing workflow.\n",
      "--- üíª GENERATING CODE ---\n",
      "Revisions remaining. Continuing workflow.\n",
      "--- üíª GENERATING CODE ---\n",
      "--- ü§î REFLECTING ON CODE ---\n",
      "--- ü§î REFLECTING ON CODE ---\n",
      "Reached maximum revisions. Ending workflow.\n",
      "\n",
      "--- ‚ú® FINAL, REFINED CODE ---\n",
      "Reached maximum revisions. Ending workflow.\n",
      "\n",
      "--- ‚ú® FINAL, REFINED CODE ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The provided code has been revised to address the critiques mentioned. Here is the updated code with explanations and improvements:\n",
       "\n",
       "```python\n",
       "import requests\n",
       "from bs4 import BeautifulSoup\n",
       "import logging\n",
       "\n",
       "# Configure logging\n",
       "logging.basicConfig(level=logging.INFO)\n",
       "logger = logging.getLogger(__name__)\n",
       "\n",
       "# Constant variables\n",
       "HACKER_NEWS_URL = \"https://news.ycombinator.com\"\n",
       "NUM_ARTICLES = 5\n",
       "\n",
       "def scrape_hacker_news(url: str = HACKER_NEWS_URL, num_articles: int = NUM_ARTICLES) -> list:\n",
       "    \"\"\"\n",
       "    Scrapes the titles of the top articles from the Hacker News homepage.\n",
       "\n",
       "    Args:\n",
       "        url (str): The URL of the Hacker News homepage. Defaults to HACKER_NEWS_URL.\n",
       "        num_articles (int): The number of articles to scrape. Defaults to NUM_ARTICLES.\n",
       "\n",
       "    Returns:\n",
       "        list: A list of article titles.\n",
       "    \"\"\"\n",
       "    try:\n",
       "        # Send a GET request to the Hacker News homepage\n",
       "        headers = {\"User-Agent\": \"Hacker News Article Scraper\"}\n",
       "        response = requests.get(url, headers=headers)\n",
       "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
       "\n",
       "        # Parse the HTML response using BeautifulSoup\n",
       "        soup = BeautifulSoup(response.text, \"lxml\")  # Using lxml parser for better performance\n",
       "\n",
       "        # Find all article titles on the page\n",
       "        titles = [item.text.strip() for item in soup.find_all(\"a\", class_=\"storylink\")]\n",
       "\n",
       "        # Return the top articles\n",
       "        return titles[:num_articles]\n",
       "\n",
       "    except requests.RequestException as e:\n",
       "        logger.error(f\"Request error: {e}\")\n",
       "        return []\n",
       "    except Exception as e:\n",
       "        logger.error(f\"An error occurred: {e}\")\n",
       "        return []\n",
       "\n",
       "def main():\n",
       "    # Example usage\n",
       "    article_titles = scrape_hacker_news()\n",
       "    for i, title in enumerate(article_titles):\n",
       "        logger.info(f\"Article {i+1}: {title}\")\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    main()\n",
       "```\n",
       "\n",
       "### Explanation of Changes\n",
       "\n",
       "1. **Error Handling**: Added try-except blocks to handle potential errors during the request or parsing process.\n",
       "2. **User-Agent Header**: Specified a User-Agent header in the request to avoid being blocked by the server.\n",
       "3. **Constant Variables**: Extracted constant variables for the URL and number of articles to improve readability and maintainability.\n",
       "4. **Type Hints**: Added type hints for function parameters and return types to enable static type checking and improve code readability.\n",
       "5. **BeautifulSoup Parser**: Changed the parser to `lxml` for better performance and handling of complex HTML structures.\n",
       "6. **Logging**: Added logging statements to help with debugging and monitoring the script's execution.\n",
       "\n",
       "### Improvements\n",
       "\n",
       "* Improved error handling and logging to make the script more robust and maintainable.\n",
       "* Used a more efficient parser for better performance.\n",
       "* Added type hints to improve code readability and enable static type checking.\n",
       "* Extracted constant variables to improve readability and maintainability.\n",
       "\n",
       "### Tests and Example Uses\n",
       "\n",
       "* Run the script to scrape the top 5 article titles from the Hacker News homepage.\n",
       "* Modify the `HACKER_NEWS_URL` and `NUM_ARTICLES` variables to scrape different numbers of articles or use a different URL.\n",
       "* Use the `logger` object to log custom messages or errors."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task = \"Create a Python function using requests and BeautifulSoup that scrapes the titles of the top 5 articles from the Hacker News homepage (https://news.ycombinator.com).\"\n",
    "initial_input = {\"task\": task} # The initial state only needs the task\n",
    "\n",
    "final_state = app.invoke(initial_input)\n",
    "\n",
    "print(\"\\n--- ‚ú® FINAL, REFINED CODE ---\")\n",
    "display(Markdown(final_state['code']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 2: Tool Calling with LangChain**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An LLM's knowledge is frozen in time and it has no access to the outside world. To build truly powerful applications, we need to give our agents **tools**‚Äîfunctions they can call to interact with APIs, databases, or any other external system.\n",
    "\n",
    "LangChain provides a seamless way to equip agents with tools and let them decide when to use them.\n",
    "\n",
    "### **Part 1: A Beginner's Guide to Tool Calling**\n",
    "\n",
    "Let's start with a very simple example: giving an agent a calculator.\n",
    "\n",
    "This will teach you the three key components of a LangChain tool-calling agent:\n",
    "1.  **The Tool:** A Python function decorated with `@tool`.\n",
    "2.  **The Agent:** The \"brain\" that decides which tool to use. We'll use `create_tool_calling_agent`.\n",
    "3.  **The AgentExecutor:** The runtime that actually executes the tool calls and passes the results back to the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 1: Define a Tool**\n",
    "\n",
    "Any Python function can become a tool. The magic is in the `@tool` decorator, which automatically converts the function's signature and docstring into a format the LLM can understand.\n",
    "\n",
    "> **Important:** A clear, descriptive docstring is crucial. The agent uses the docstring to figure out *what the tool does* and *when to use it*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies two integers together.\"\"\"\n",
    "    print(\"Bro needs to multiply\", a, \"and\", b)\n",
    "    return a * b\n",
    "\n",
    "# We create a list of all the tools the agent will have access to.\n",
    "tools = [multiply]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 2: Create a Tool-Calling Agent and Executor**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we assemble the agent. We need the LLM, our list of tools, and a special prompt.\n",
    "\n",
    "The prompt is the agent's instruction manual. We'll use a pre-built template from LangChain which includes a special placeholder: `agent_scratchpad`. This is where the agent will keep track of its internal thoughts and previous tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatGroq(api_key = os.getenv(\"GROQ_API_KEY\"),model=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "# The agent is created with the model, tools, and a system prompt.\n",
    "# This single object is now the complete, runnable agent.\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    system_prompt=\"You are a helpful assistant. You must use your tools to answer questions.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 3: Run the Agent**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask a question that requires the agent to use its `multiply` tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro needs to multiply 8 and 7\n",
      "The answer to 8 times 7 is 56.\n",
      "The answer to 8 times 7 is 56.\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke({\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is 8 times 7?\"}\n",
    "    ]\n",
    "})\n",
    "\n",
    "# The final answer is in the 'content' of the last message in the output.\n",
    "final_answer = result[\"messages\"][-1].content\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 2: Your Task - Build a Multi-Tool Travel Agent [10 marks]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use the modern `create_agent` function to build a \"Travel Agent\" that can use **multiple tools** to answer a complex user query.\n",
    "\n",
    "**Goal:** Create a single agent that can help a user plan a trip by providing information on flights, weather, and local events.\n",
    "\n",
    "Follow the `TODO` comments below to implement the full agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`TODO 1`: Create the Tools [5 marks]**\n",
    "Define three distinct Python functions. Since we don't have real APIs for this, you will create **mock functions** that return hardcoded string data. Each function must have a clear docstring explaining what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_flight_info(origin: str, destination: str, month: str) -> str:\n",
    "    \"\"\"Provides fictional flight prices and availability for a trip.\"\"\"\n",
    "    return json.dumps({\n",
    "        \"origin\": origin,\n",
    "        \"destination\": destination,\n",
    "        \"month\": month,\n",
    "        \"flights\": [\n",
    "            {\"airline\": \"Pakistan International Airlines\", \"price\": 350, \"availability\": \"Available\"},\n",
    "            {\"airline\": \"FlyJinnah\", \"price\": 400, \"availability\": \"Limited Seats\"},\n",
    "            {\"airline\": \"Air Sial\", \"price\": 300, \"availability\": \"Sold Out\"}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "@tool\n",
    "def get_weather_forecast(city: str, month: str) -> str:\n",
    "    \"\"\"Provides a fictional weather forecast for a specific city and month.\"\"\"\n",
    "    return json.dumps({\n",
    "        \"city\": city,\n",
    "        \"month\": month,\n",
    "        \"forecast\": {\n",
    "            \"average_high\": \"30¬∞C\",\n",
    "            \"average_low\": \"20¬∞C\",\n",
    "            \"precipitation\": \"50mm\"\n",
    "        }\n",
    "    })\n",
    "\n",
    "@tool\n",
    "def search_city_events(city: str, month: str) -> str:\n",
    "    \"\"\"Provides a list of major fictional events for a specific city and month.\"\"\"\n",
    "    return json.dumps({\n",
    "        \"city\": city,\n",
    "        \"month\": month,\n",
    "        \"events\": [\n",
    "            {\"name\": \"Hasan Raheem Concert\", \"date\": f\"{month}-15\", \"description\": \"A grand music festival featuring Hasan Raheem.\"},\n",
    "            {\"name\": \"Pakwheels Car Mela\", \"date\": f\"{month}-22\", \"description\": \"A car mela where you can buy and sell vehicles.\"},\n",
    "            {\"name\": \"Topics in LLMs Final Presentation\", \"date\": f\"{month}-28\", \"description\": \"The final...\"}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "travel_tools = [get_flight_info, get_weather_forecast, search_city_events]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`TODO 2`: Create and Run the Agent üöÄ [5 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For your trip to Tel Aviv from Tehran in June, there are several flights available with prices ranging from $300 to $400. The weather in Tel Aviv during June is expected to be warm with average highs of 30¬∞C and average lows of 20¬∞C, with a precipitation of 50mm. As for major events, there are a few happening in Tel Aviv during June, including the Hasan Raheem Concert on June 15th, the Pakwheels Car Mela on June 22nd, and the Topics in LLMs Final Presentation on June 28th.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# 1. Initialize the LLM \n",
    "llm = ChatGroq(api_key = os.getenv(\"GROQ_API_KEY\"),model=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "# 2. Define a clear and concise system prompt for your travel agent.\n",
    "system_prompt = \"You are a travel planning assistant. Use the available tools to help users plan their trips by providing information on flights, weather, and local events.\"\n",
    "\n",
    "# 3. Create the agent using create_agent, passing the llm, tools list, and system prompt.\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=travel_tools,\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "user_message_content = \"Help me plan a trip to Tel Aviv from Tehran for this June. I need to know about flights, weather, and any major events.\"\n",
    "\n",
    "# 5. Invoke the agent with the correct message format and print the final answer.\n",
    "result = agent.invoke({\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": user_message_content}\n",
    "    ]\n",
    "})\n",
    "final_answer = result[\"messages\"][-1].content\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 3: Advanced Multi-Agent Collaboration with LangGraph**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now mastered the core patterns of agentic design: stateful workflows with `LangGraph`, tool use with `create_agent`, and multi-agent collaboration. This final project will challenge you to combine all these skills to build a sophisticated, practical research crew.\n",
    "\n",
    "Your goal is to create a multi-agent system that can verify a claim by consulting multiple sources, cross-referencing their findings, and looping its research until it reaches a confident conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using the Tavily Search Tool**\n",
    "For this task, we will use the **Tavily Search API** for live web searches. It is a powerful tool designed specifically for LLM agents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tavily.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# It is recommended to use a secrets manager for your keys.\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, you're free to use any model here on. But to warn you, several models (e.g. LLama we used earlier) are absolutely terrible at calling tools appropriately. Mistral should work fine, most of the times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "from langchain.agents import create_agent\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# You can configure the tool's parameters, like max_results, upon instantiation\n",
    "search_tool = TavilySearch(max_results=3)\n",
    "\n",
    "\n",
    "# You can then pass this tool in a list to your agent like this and it will automatically perform a search if needed\n",
    "llm = ChatMistralAI(api_key=os.getenv(\"MISTRAL_API_KEY\"), model=\"mistral-small-latest\")\n",
    "my_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[search_tool], # Pass the tool here\n",
    "    system_prompt=\"You are a web researcher.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Your Task: Build the Fact-Checking Crew [30 marks]**\n",
    "\n",
    "#### **High-Level Architecture**\n",
    "\n",
    "Your crew will now include a skeptical agent to ensure a balanced perspective.\n",
    "1.  **Start with a claim.**\n",
    "2.  **Initial Research:** A news-focused agent will search for evidence supporting the claim.\n",
    "3.  **Adversarial Research:** A \"Devil's Advocate\" agent will actively search for evidence that *contradicts* the claim.\n",
    "4.  **Verification:** A lead verifier will analyze both the supporting and contradictory evidence, produce a consolidated analysis, and decide on a verdict: `CONFIRMED`, `CONTRADICTED`, or `NEEDS_MORE_INFO`.\n",
    "5.  **Conditional Loop:** If the verdict is `NEEDS_MORE_INFO`, the graph loops back for another round of research.\n",
    "6.  **Final Report:** A writer agent takes the final, verified analysis and produces a polished report.\n",
    "\n",
    "#### **`TODO 1`: Define the Graph State [5 marks]**\n",
    "The state needs to track the claim, the findings from the pro and con agents, the verifier's analysis, and the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "\n",
    "class FactCheckCrewState(TypedDict):\n",
    "    claim: str\n",
    "    revision_number: int\n",
    "    supporting_evidence: str\n",
    "    counter_evidence: str\n",
    "    verified_analysis: str\n",
    "    final_verdict: str\n",
    "    final_report: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`TODO 2`: Define the Agent Nodes [15 marks]**\n",
    "You will now create four distinct agent nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_tavily import TavilySearch\n",
    "import json\n",
    "\n",
    "#Use this search tool to enable live searches\n",
    "search_tool = TavilySearch(max_results=4)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatMistralAI(api_key=os.getenv(\"MISTRAL_API_KEY\"), model=\"mistral-small-latest\")\n",
    "\n",
    "# --- SUPPORTING EVIDENCE AGENT ---\n",
    "def supporting_evidence_node(state: FactCheckCrewState):\n",
    "    \"\"\"Agent 1: Uses the search tool to find recent news and articles that SUPPORT the claim.\"\"\"\n",
    "    print(\"--- üîé SUPPORTING EVIDENCE NODE ---\")\n",
    "    support_agent = create_agent(\n",
    "        model=ChatMistralAI(api_key=os.getenv(\"MISTRAL_API_KEY\"), model=\"mistral-small-latest\"),\n",
    "        tools=[search_tool],\n",
    "        system_prompt=\"You are a research assistant. Find credible news, studies, and expert opinions that support the claim.\"\n",
    "    )\n",
    "    result = support_agent.invoke({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": state[\"claim\"]}\n",
    "        ]\n",
    "    })\n",
    "    supporting_evidence = result[\"messages\"][-1].content\n",
    "    return {\"supporting_evidence\": supporting_evidence}\n",
    "\n",
    "# --- DEVIL'S ADVOCATE AGENT ---\n",
    "def devils_advocate_node(state: FactCheckCrewState):\n",
    "    \"\"\"Agent 2: Uses the search tool to find evidence that CONTRADICTS the claim.\"\"\"\n",
    "    print(\"--- üòà DEVIL'S ADVOCATE NODE ---\")\n",
    "    devil_agent = create_agent(\n",
    "        model=ChatMistralAI(api_key=os.getenv(\"MISTRAL_API_KEY\"), model=\"mistral-small-latest\"),\n",
    "        tools=[search_tool],\n",
    "        system_prompt=\"You are a Devil's Advocate. Find counter-arguments, dissenting opinions, and evidence that debunks or challenges the claim.\"\n",
    "    )\n",
    "    result = devil_agent.invoke({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": state[\"claim\"]}\n",
    "        ]\n",
    "    })\n",
    "    counter_evidence = result[\"messages\"][-1].content\n",
    "    return {\"counter_evidence\": counter_evidence}\n",
    "\n",
    "# --- LEAD VERIFIER AGENT ---\n",
    "def lead_verifier_node(state: FactCheckCrewState):\n",
    "    \"\"\"Agent 3: Synthesizes both sides and makes a verdict. Does not use tools.\"\"\"\n",
    "    print(\"--- ‚öñÔ∏è LEAD VERIFIER NODE ---\")\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"You are a neutral judge. Weigh the following evidence:\\n\\n\"\n",
    "        \"Supporting Evidence:\\n{supporting_evidence}\\n\\n\"\n",
    "        \"Counter Evidence:\\n{counter_evidence}\\n\\n\"\n",
    "        \"You MUST respond with ONLY a valid JSON object (no other text before or after). \"\n",
    "        \"The JSON must have exactly two keys:\\n\"\n",
    "        '- \"analysis\": A summary of the verified findings.\\n'\n",
    "        '- \"verdict\": One of exactly these three values: \"CONFIRMED\", \"CONTRADICTED\", or \"NEEDS_MORE_INFO\".\\n\\n'\n",
    "        \"Example format:\\n\"\n",
    "        '{{\"analysis\": \"Your analysis here\", \"verdict\": \"CONFIRMED\"}}'\n",
    "        'This format MUST be followed strictly.'\n",
    "    )\n",
    "    chain = prompt | ChatMistralAI(api_key=os.getenv(\"MISTRAL_API_KEY\"), model=\"mistral-small-latest\")\n",
    "    verification_result = chain.invoke({\n",
    "        \"supporting_evidence\": state[\"supporting_evidence\"],\n",
    "        \"counter_evidence\": state[\"counter_evidence\"]\n",
    "    })\n",
    "    # Clean up the response - remove markdown code blocks if present\n",
    "    content = verification_result.content.strip()\n",
    "    if content.startswith(\"```json\"):\n",
    "        content = content[7:]  # Remove ```json\n",
    "    if content.startswith(\"```\"):\n",
    "        content = content[3:]  # Remove ```\n",
    "    if content.endswith(\"```\"):\n",
    "        content = content[:-3]  # Remove trailing ```\n",
    "    content = content.strip()\n",
    "    \n",
    "    output_json = json.loads(content)\n",
    "    return {\n",
    "        \"verified_analysis\": output_json[\"analysis\"],\n",
    "        \"final_verdict\": output_json[\"verdict\"],\n",
    "        \"revision_number\": state[\"revision_number\"] + 1\n",
    "    }\n",
    "\n",
    "# --- REPORT WRITER AGENT ---\n",
    "def report_writer_node(state: FactCheckCrewState):\n",
    "    \"\"\"Agent 4: Writes the final, polished report.\"\"\"\n",
    "    print(\"--- üñãÔ∏è REPORT WRITER NODE ---\")\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"You are a report writer. Based on the following analysis and verdict, write a clear, neutral, and well-structured report for a general audience:\\n\\n\"\n",
    "        \"Analysis:\\n{verified_analysis}\\n\\n\"\n",
    "        \"Verdict:\\n{final_verdict}\"\n",
    "    )\n",
    "    chain = prompt | ChatMistralAI(api_key=os.getenv(\"MISTRAL_API_KEY\"), model=\"mistral-small-latest\")\n",
    "    report_result = chain.invoke({\n",
    "        \"verified_analysis\": state[\"verified_analysis\"],\n",
    "        \"final_verdict\": state[\"final_verdict\"]\n",
    "    })\n",
    "    return {\"final_report\": report_result.content}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`TODO 3`: Implement the Conditional Edge [5 marks]**\n",
    "This function will read the `final_verdict` and decide the next step for the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue_verification(state: FactCheckCrewState):\n",
    "    \"\"\"Determines whether to loop back for more research or finish.\"\"\"\n",
    "    print(\"--- üîÅ DECISION NODE ---\")\n",
    "    final_verdict = state[\"final_verdict\"]\n",
    "    revision_number = state[\"revision_number\"]\n",
    "    if final_verdict == \"NEEDS_MORE_INFO\" and revision_number < 2:\n",
    "        print(\"More research needed. Continuing verification loop.\")\n",
    "        return \"continue_research\"\n",
    "    else:\n",
    "        print(\"Sufficient information gathered. Finishing report.\")\n",
    "        return \"finish_report\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`TODO 4`: Wire Up the Graph [5 marks]**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(FactCheckCrewState)\n",
    "workflow.add_node(\"supporting_evidence\", supporting_evidence_node)\n",
    "workflow.add_node(\"devils_advocate\", devils_advocate_node)\n",
    "workflow.add_node(\"lead_verifier\", lead_verifier_node)\n",
    "workflow.add_node(\"report_writer\", report_writer_node)\n",
    "workflow.set_entry_point(\"supporting_evidence\")\n",
    "workflow.add_edge(\"supporting_evidence\", \"devils_advocate\")\n",
    "workflow.add_edge(\"devils_advocate\", \"lead_verifier\")\n",
    "workflow.add_conditional_edges(\"lead_verifier\", should_continue_verification, {\"continue_research\": \"supporting_evidence\", \"finish_report\": \"report_writer\"})\n",
    "workflow.add_edge(\"report_writer\", END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Run Your Completed Crew**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üîé SUPPORTING EVIDENCE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "Sufficient information gathered. Finishing report.\n",
      "--- üñãÔ∏è REPORT WRITER NODE ---\n",
      "\n",
      "\n",
      "--- ‚úÖ FINAL REPORT ---\n",
      "**Report: Confirmation of Pakistan's Independence**\n",
      "\n",
      "**Introduction**\n",
      "\n",
      "This report aims to provide a clear and concise overview of the historical fact of Pakistan's independence. Following a thorough analysis of available evidence and historical records, we are able to confirm the country's independence in 1947.\n",
      "\n",
      "**Key Findings**\n",
      "\n",
      "The evidence confirms that Pakistan gained independence on August 14, 1947. This date is widely recognized and supported by historical events, including the swearing-in of Muhammad Ali Jinnah as the first governor general of Pakistan. Additionally, the release of commemorative postage stamps on this occasion further substantiates the fact of Pakistan's independence.\n",
      "\n",
      "**Historical Context**\n",
      "\n",
      "While there may be ongoing discussions and debates about the nature and circumstances surrounding Pakistan's independence, our analysis focuses on the core fact of the country's independence in 1947. The historical records and events of that time provide a clear indication of Pakistan's transition to an independent nation.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "Based on the analysis of historical evidence and events, we confirm that Pakistan gained independence in 1947, with August 14, 1947, being the established independence day. This report aims to provide a neutral and factual account of this significant historical event, and we hope it will serve as a reliable source of information for the general audience. The verdict on this matter is therefore: **CONFIRMED**.\n"
     ]
    }
   ],
   "source": [
    "claim = \"Pakistan gained independence in 1947.\"\n",
    "initial_input = {\"claim\": claim, \"revision_number\": 0}\n",
    "\n",
    "final_state = app.invoke(initial_input)\n",
    "\n",
    "print(\"\\n\\n--- ‚úÖ FINAL REPORT ---\")\n",
    "print(final_state['final_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 4: A Comparative Study of Fact-Checking Agents** [20 marks]\n",
    "\n",
    "How much better is a complex agent than a simple one? You will answer that question by empirically evaluating three different fact-checking methods against a real-world dataset of claims.\n",
    "\n",
    "#### **The Goal**\n",
    "\n",
    "You will take a dataset of fact-checked claims and run each claim through three different verifiers:\n",
    "1.  **Method 1: The Zero-Shot LLM:** A baseline agent with no tools, relying solely on its internal knowledge.\n",
    "2.  **Method 2: The Simple Search Agent:** A single agent equipped with a web search tool (a basic RAG approach).\n",
    "3.  **Method 3: The Advanced Research Crew:** The multi-agent, adversarial fact-checking crew you just built.\n",
    "\n",
    "Finally, you will compare the accuracy of each method to determine the value of agentic complexity.\n",
    "\n",
    "#### **Setup: Loading the Dataset**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('claims.csv')\n",
    "    claims_sample = df[:10].copy()\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'claims.csv' not found.\")\n",
    "    claims_sample = pd.DataFrame()\n",
    "\n",
    "def normalize_verdict(verdict):\n",
    "    if verdict:\n",
    "        return \"true\"\n",
    "    else:\n",
    "        return \"false\"\n",
    "    \n",
    "\n",
    "if not claims_sample.empty:\n",
    "    claims_sample['ground_truth'] = claims_sample['text review'].apply(normalize_verdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You might want to enforce structured outputs to ensure your final answer is a True/False: <br> \n",
    "https://docs.langchain.com/oss/python/langchain/structured-output <br>\n",
    "https://forum.langchain.com/t/make-a-llm-with-structured-output-call-a-tool/622"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 1: Baseline Fact-Checker (Zero-Shot LLM) [5 marks]**\n",
    "\n",
    "This agent has no access to the outside world. It will make its judgment based only on the information it was trained on.\n",
    "\n",
    "**Your Task:** Implement the `verify_claim_zero_shot` function. This function should use a simple LLM chain to classify a claim as \"True\" or \"False\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Zero-Shot Verifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zero-Shot Verification: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:03<00:00,  3.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# All our necessary imports from previous sections\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "llm = ChatMistralAI(api_key=os.getenv(\"MISTRAL_API_KEY\"), model=\"mistral-small-latest\")\n",
    "search_tool = TavilySearch(max_results=4)\n",
    "\n",
    "def verify_claim_zero_shot(claim: str) -> str:\n",
    "    \"\"\"Verifies a claim using only the LLM's internal knowledge.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"You are a fact-checker. Verify the following claim:\\n\\n{claim}\\n\\n\"\n",
    "        \"Respond with ONLY the word 'True' if the claim is correct, or 'False' if it is incorrect.\"\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    result = chain.invoke({\"claim\": claim})\n",
    "    output = result.content.strip().lower()\n",
    "    if \"true\" in output:\n",
    "        return \"True\"\n",
    "    else:\n",
    "        return \"False\"\n",
    "\n",
    "# --- Run the evaluation ---\n",
    "print(\"Running Zero-Shot Verifier...\")\n",
    "results_zero_shot = []\n",
    "for claim in tqdm(claims_sample['claim'], desc=\"Zero-Shot Verification\"):\n",
    "    verdict = verify_claim_zero_shot(claim)\n",
    "    results_zero_shot.append(verdict)\n",
    "\n",
    "claims_sample['zero_shot_verdict'] = results_zero_shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 2: Simple RAG Fact-Checker (Agent with Search Tool) [5 marks]**\n",
    "This agent represents a standard RAG (Retrieval-Augmented Generation) approach. It can search the web for information before making a decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Task:** Implement the `verify_claim_with_search` function. This function will create a simple agent equipped with the `TavilySearch` tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Simple Search Verifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Search Verification: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [04:29<00:00, 27.00s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "llm = ChatMistralAI(api_key=os.getenv(\"MISTRAL_API_KEY\"), model=\"mistral-small-latest\")\n",
    "\n",
    "def verify_claim_with_search(claim: str) -> str:\n",
    "    \"\"\"Verifies a claim using a single agent with a web search tool.\"\"\"\n",
    "    agent = create_agent(\n",
    "        model=llm,\n",
    "        tools=[search_tool],\n",
    "        system_prompt=\"You are a fact-checker with access to a web search tool. Use the tool to find information and make a final judgment on the claim. You MUST respond with ONLY the word 'True' or 'False'.\"\n",
    "    )\n",
    "    result = agent.invoke({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": claim}\n",
    "        ]\n",
    "    })\n",
    "    final_output = result[\"messages\"][-1].content.strip().lower()\n",
    "    if \"true\" in final_output:\n",
    "        return \"True\"\n",
    "    else:\n",
    "        return \"False\"\n",
    "\n",
    "# --- Run the evaluation ---\n",
    "print(\"\\nRunning Simple Search Verifier...\")\n",
    "results_with_search = []\n",
    "for claim in tqdm(claims_sample['claim'], desc=\"Search Verification\"):\n",
    "    verdict = verify_claim_with_search(claim)\n",
    "    results_with_search.append(verdict)\n",
    "    time.sleep(20)  # Add a delay to avoid hitting rate limits\n",
    "\n",
    "\n",
    "claims_sample['simple_search_verdict'] = results_with_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 3: Advanced Fact-Checker (Multi-Agent Crew) [5 marks]**\n",
    "\n",
    "**You may re-use the multi-agent crew you built in the previous section.**\n",
    "\n",
    "**Your Task:** Implement the `verify_claim_with_crew` function. This function will invoke your crew and, most importantly, translate its complex output (`CONFIRMED`, `CONTRADICTED`, `NEEDS_MORE_INFO`) into the simple \"True\"/\"False\" format required for evaluation.\n",
    "\n",
    "Note: Restrict the number of revisions to 2. If the crew is not able to arrive at a definitive answer, output 'uncertain'.  I'll leave it upto you to think and decide if such a behavior should be penalized. You may compute your accuracy accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Multi-Agent Crew Verifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crew Verification:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üîé SUPPORTING EVIDENCE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "More research needed. Continuing verification loop.\n",
      "--- üîé SUPPORTING EVIDENCE NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "More research needed. Continuing verification loop.\n",
      "--- üîé SUPPORTING EVIDENCE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "Sufficient information gathered. Finishing report.\n",
      "--- üñãÔ∏è REPORT WRITER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "Sufficient information gathered. Finishing report.\n",
      "--- üñãÔ∏è REPORT WRITER NODE ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crew Verification:  10%|‚ñà         | 1/10 [01:33<13:58, 93.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üîé SUPPORTING EVIDENCE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "More research needed. Continuing verification loop.\n",
      "--- üîé SUPPORTING EVIDENCE NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "More research needed. Continuing verification loop.\n",
      "--- üîé SUPPORTING EVIDENCE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_mistralai.chat_models.ChatMistralAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ReadTimeout: The read operation timed out.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "Sufficient information gathered. Finishing report.\n",
      "--- üñãÔ∏è REPORT WRITER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "Sufficient information gathered. Finishing report.\n",
      "--- üñãÔ∏è REPORT WRITER NODE ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crew Verification:  20%|‚ñà‚ñà        | 2/10 [04:52<20:46, 155.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üîé SUPPORTING EVIDENCE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "More research needed. Continuing verification loop.\n",
      "--- üîé SUPPORTING EVIDENCE NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "More research needed. Continuing verification loop.\n",
      "--- üîé SUPPORTING EVIDENCE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "Sufficient information gathered. Finishing report.\n",
      "--- üñãÔ∏è REPORT WRITER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "Sufficient information gathered. Finishing report.\n",
      "--- üñãÔ∏è REPORT WRITER NODE ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crew Verification:  30%|‚ñà‚ñà‚ñà       | 3/10 [06:31<15:08, 129.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üîé SUPPORTING EVIDENCE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "Sufficient information gathered. Finishing report.\n",
      "--- üñãÔ∏è REPORT WRITER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "Sufficient information gathered. Finishing report.\n",
      "--- üñãÔ∏è REPORT WRITER NODE ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crew Verification:  40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [07:46<10:48, 108.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üîé SUPPORTING EVIDENCE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "More research needed. Continuing verification loop.\n",
      "--- üîé SUPPORTING EVIDENCE NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "More research needed. Continuing verification loop.\n",
      "--- üîé SUPPORTING EVIDENCE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "Sufficient information gathered. Finishing report.\n",
      "--- üñãÔ∏è REPORT WRITER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "Sufficient information gathered. Finishing report.\n",
      "--- üñãÔ∏è REPORT WRITER NODE ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crew Verification:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [09:15<08:26, 101.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üîé SUPPORTING EVIDENCE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "Sufficient information gathered. Finishing report.\n",
      "--- üñãÔ∏è REPORT WRITER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "Sufficient information gathered. Finishing report.\n",
      "--- üñãÔ∏è REPORT WRITER NODE ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crew Verification:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [10:21<05:57, 89.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üîé SUPPORTING EVIDENCE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "Sufficient information gathered. Finishing report.\n",
      "--- üñãÔ∏è REPORT WRITER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "Sufficient information gathered. Finishing report.\n",
      "--- üñãÔ∏è REPORT WRITER NODE ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crew Verification:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [11:32<04:10, 83.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üîé SUPPORTING EVIDENCE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "More research needed. Continuing verification loop.\n",
      "--- üîé SUPPORTING EVIDENCE NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "More research needed. Continuing verification loop.\n",
      "--- üîé SUPPORTING EVIDENCE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "Sufficient information gathered. Finishing report.\n",
      "--- üñãÔ∏è REPORT WRITER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "Sufficient information gathered. Finishing report.\n",
      "--- üñãÔ∏è REPORT WRITER NODE ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crew Verification:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [13:10<02:55, 87.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üîé SUPPORTING EVIDENCE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "Sufficient information gathered. Finishing report.\n",
      "--- üñãÔ∏è REPORT WRITER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "Sufficient information gathered. Finishing report.\n",
      "--- üñãÔ∏è REPORT WRITER NODE ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crew Verification:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [14:25<01:24, 84.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üîé SUPPORTING EVIDENCE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- üòà DEVIL'S ADVOCATE NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- ‚öñÔ∏è LEAD VERIFIER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "Sufficient information gathered. Finishing report.\n",
      "--- üñãÔ∏è REPORT WRITER NODE ---\n",
      "--- üîÅ DECISION NODE ---\n",
      "Sufficient information gathered. Finishing report.\n",
      "--- üñãÔ∏è REPORT WRITER NODE ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crew Verification: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [15:41<00:00, 94.16s/it]\n",
      "Crew Verification: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [15:41<00:00, 94.16s/it]\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "\n",
    "llm = ChatMistralAI(api_key=os.getenv(\"MISTRAL_API_KEY\"), model=\"mistral-small-latest\")\n",
    "\n",
    "workflow = StateGraph(FactCheckCrewState)\n",
    "workflow.add_node(\"supporting_evidence\", supporting_evidence_node)\n",
    "workflow.add_node(\"devils_advocate\", devils_advocate_node)\n",
    "workflow.add_node(\"lead_verifier\", lead_verifier_node)\n",
    "workflow.add_node(\"report_writer\", report_writer_node)\n",
    "workflow.set_entry_point(\"supporting_evidence\")\n",
    "workflow.add_edge(\"supporting_evidence\", \"devils_advocate\")\n",
    "workflow.add_edge(\"devils_advocate\", \"lead_verifier\")\n",
    "workflow.add_conditional_edges(\"lead_verifier\", should_continue_verification, {\"continue_research\": \"supporting_evidence\", \"finish_report\": \"report_writer\"})\n",
    "workflow.add_edge(\"report_writer\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "def verify_claim_with_crew(claim: str) -> str:\n",
    "    \"\"\"Verifies a claim using the full multi-agent research crew.\"\"\"\n",
    "    initial_state = {\"claim\": claim, \"revision_number\": 0}\n",
    "    final_state = app.invoke(initial_state)\n",
    "    final_verdict = final_state[\"final_verdict\"]\n",
    "    if final_verdict == \"CONFIRMED\":\n",
    "        return \"True\"\n",
    "    elif final_verdict == \"CONTRADICTED\":\n",
    "        return \"False\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "# --- Run the evaluation ---\n",
    "print(\"\\nRunning Multi-Agent Crew Verifier...\")\n",
    "results_with_crew = []\n",
    "# Ensure your crew's LangGraph `app` is defined and compiled in a cell above this one!\n",
    "for claim in tqdm(claims_sample['claim'], desc=\"Crew Verification\"):\n",
    "    verdict = verify_claim_with_crew(claim)\n",
    "    results_with_crew.append(verdict)\n",
    "    time.sleep(60)\n",
    "\n",
    "claims_sample['crew_verdict'] = results_with_crew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Final Evaluation & Analysis [5 marks]**\n",
    "\n",
    "Now for the moment of truth. We will calculate the accuracy of each method by comparing its verdicts to the ground truth from the dataset.\n",
    "\n",
    "**Your Task:** Run the evaluation code and then, in the final markdown cell, write a brief analysis of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Results ---\n",
      "Zero-Shot LLM Accuracy: 80.00%\n",
      "Simple Search Agent Accuracy: 90.00%\n",
      "Multi-Agent Crew Accuracy: 40.00%\n",
      "\n",
      "--- Detailed Comparison ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>zero_shot_verdict</th>\n",
       "      <th>simple_search_verdict</th>\n",
       "      <th>crew_verdict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do researchers predict 'Medicare for All' woul...</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fotoƒürafƒ±n Binali Yƒ±ldƒ±rƒ±m'a verilen tablodaki...</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abimola, 36 anni, congolese pregiudicato, vive...</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Most affected countries by COVID-19 are along ...</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Interven√ß√£o art√≠stica em Arroios custou 37 mil...</td>\n",
       "      <td>true</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Donald Trump Pakistan‚Äôda mƒ± doƒüdu?</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>An image shared on Instagram purportedly shows...</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>‡¥∏‡µº‡¥µ‡µº‡¥ï‡µç‡¥ï‡µº ‡¥Æ‡¥æ‡¥™‡µç‡¥™‡µç ‡¥™‡¥±‡¥û‡µç‡¥û‡µÜ‡¥®‡µç‡¥®‡µç 1947 ‡µΩ ‡¥ú‡¥®‡¥≠‡µÇ‡¥Æ‡¥ø ‡¥µ‡¥æ‡µº‡¥§‡µç‡¥§</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Photograph shows an eagle perched atop a grave...</td>\n",
       "      <td>true</td>\n",
       "      <td>false</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Infiltrado do MST √© gravado quebrando vidra√ßas...</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>false</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim ground_truth  \\\n",
       "0  Do researchers predict 'Medicare for All' woul...         true   \n",
       "1  Fotoƒürafƒ±n Binali Yƒ±ldƒ±rƒ±m'a verilen tablodaki...        false   \n",
       "2  Abimola, 36 anni, congolese pregiudicato, vive...        false   \n",
       "3  Most affected countries by COVID-19 are along ...        false   \n",
       "4  Interven√ß√£o art√≠stica em Arroios custou 37 mil...         true   \n",
       "5                 Donald Trump Pakistan‚Äôda mƒ± doƒüdu?        false   \n",
       "6  An image shared on Instagram purportedly shows...        false   \n",
       "7    ‡¥∏‡µº‡¥µ‡µº‡¥ï‡µç‡¥ï‡µº ‡¥Æ‡¥æ‡¥™‡µç‡¥™‡µç ‡¥™‡¥±‡¥û‡µç‡¥û‡µÜ‡¥®‡µç‡¥®‡µç 1947 ‡µΩ ‡¥ú‡¥®‡¥≠‡µÇ‡¥Æ‡¥ø ‡¥µ‡¥æ‡µº‡¥§‡µç‡¥§        false   \n",
       "8  Photograph shows an eagle perched atop a grave...         true   \n",
       "9  Infiltrado do MST √© gravado quebrando vidra√ßas...        false   \n",
       "\n",
       "  zero_shot_verdict simple_search_verdict crew_verdict  \n",
       "0              true                  true         true  \n",
       "1             false                 false        other  \n",
       "2             false                 false        other  \n",
       "3             false                 false        false  \n",
       "4             false                 false        other  \n",
       "5             false                 false         true  \n",
       "6             false                 false        false  \n",
       "7             false                 false        other  \n",
       "8             false                  true         true  \n",
       "9             false                 false         true  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make all the final verdict columns all lowercase for consistency\n",
    "claims_sample['zero_shot_verdict'] = claims_sample['zero_shot_verdict'].str.lower()\n",
    "claims_sample['simple_search_verdict'] = claims_sample['simple_search_verdict'].str.lower()\n",
    "claims_sample['crew_verdict'] = claims_sample['crew_verdict'].str.lower()\n",
    "\n",
    "# --- Calculate Accuracy ---\n",
    "ground_truth = claims_sample['ground_truth'].values\n",
    "\n",
    "accuracy_zero_shot = (claims_sample['zero_shot_verdict'].values == ground_truth).mean()\n",
    "accuracy_simple_search = (claims_sample['simple_search_verdict'].values == ground_truth).mean()\n",
    "accuracy_crew = (claims_sample['crew_verdict'].values == ground_truth).mean()\n",
    "\n",
    "print(\"--- Final Results ---\")\n",
    "print(f\"Zero-Shot LLM Accuracy: {accuracy_zero_shot:.2%}\")\n",
    "print(f\"Simple Search Agent Accuracy: {accuracy_simple_search:.2%}\")\n",
    "print(f\"Multi-Agent Crew Accuracy: {accuracy_crew:.2%}\")\n",
    "\n",
    "print(\"\\n--- Detailed Comparison ---\")\n",
    "display(claims_sample[['claim', 'ground_truth', 'zero_shot_verdict', 'simple_search_verdict', 'crew_verdict']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Analysis of Results**\n",
    "\n",
    "**1. Which method performed the best? Why?**\n",
    "\n",
    "The Simple Search Agent performed the best with 90% accuracy, followed by Zero-shot LLM at 80%, while the Multi-Agent Crew underperformed at 40%. \n",
    "\n",
    "The Simple Search Agent's success stems from its balanced and streamlined approach: it combines the LLM's reasoning with real-time web access for claim verification. Its singular focus (search $\\rightarrow$ analyze $\\rightarrow$ decide) is efficient and avoids the errors introduced by complex multi-agent handoffs.\n",
    "\n",
    "**2. Do you think these results were expected?**\n",
    "\n",
    "The results are unexpected, particularly the Multi-Agent Crew's low performance. Theoretically, its adversarial reasoning (supporting vs. contradicting evidence) should improve performance.\n",
    "\n",
    "However, factors contributing to the underperformance likely include:\n",
    "\n",
    "- **Increased complexity:** More agents and handoffs introduce more failure points and opportunities for miscommunication.\n",
    "\n",
    "- **Verdict translation challenges:** The crew's nuanced verdicts (CONFIRMED/CONTRADICTED/NEEDS_MORE_INFO) don't map cleanly to a binary True/False, especially when uncertain.\n",
    "\n",
    "- **Conflicting evidence:** The adversarial design may find contradictory information even for straightforward claims, leading to false \"NEEDS_MORE_INFO\" verdicts.\n",
    "\n",
    "The Simple Search Agent's superior performance suggests that for binary fact-checking, a focused single-agent approach with tool access is often more effective than complex multi-agent architectures.\n",
    "\n",
    "**3. If the multi-agent crew was not able to output a final answer - what's better from a social welfare perspective: an output you are not confident about or refraining from giving an output if you are not confident?**\n",
    "\n",
    "From a social welfare perspective, refraining from giving an output when confidence is low is generally the more ethical choice. It prevents misinformation and builds long-term trust. A confident incorrect verdict is more harmful than admitting uncertainty."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
