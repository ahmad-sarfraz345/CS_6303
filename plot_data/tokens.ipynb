{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23062cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tiktoken\n",
    "%pip install huggingface-hub\n",
    "%pip install datasets\n",
    "# login using hf auth login (in the terminal) and paste your token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71d2d237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating DZ split: 500 examples [00:00, 19035.60 examples/s]\n",
      "Generating DZ split: 500 examples [00:00, 19035.60 examples/s]\n",
      "Generating AS split: 500 examples [00:00, 11681.67 examples/s]\n",
      "Generating AS split: 500 examples [00:00, 11681.67 examples/s]\n",
      "Generating AZ split: 0 examples [00:00, ? examples/s]\n",
      "\n"
     ]
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\coding\\CS_6303\\plot_data\\.venv\\Lib\\site-packages\\datasets\\builder.py:1871\u001b[39m, in \u001b[36mArrowBasedBuilder._prepare_split_single\u001b[39m\u001b[34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[39m\n\u001b[32m   1870\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1871\u001b[39m     \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1872\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m CastError \u001b[38;5;28;01mas\u001b[39;00m cast_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\coding\\CS_6303\\plot_data\\.venv\\Lib\\site-packages\\datasets\\arrow_writer.py:643\u001b[39m, in \u001b[36mArrowWriter.write_table\u001b[39m\u001b[34m(self, pa_table, writer_batch_size)\u001b[39m\n\u001b[32m    642\u001b[39m pa_table = pa_table.combine_chunks()\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m pa_table = \u001b[43mtable_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embed_local_files:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\coding\\CS_6303\\plot_data\\.venv\\Lib\\site-packages\\datasets\\table.py:2293\u001b[39m, in \u001b[36mtable_cast\u001b[39m\u001b[34m(table, schema)\u001b[39m\n\u001b[32m   2292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m table.schema != schema:\n\u001b[32m-> \u001b[39m\u001b[32m2293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast_table_to_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2294\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m table.schema.metadata != schema.metadata:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\coding\\CS_6303\\plot_data\\.venv\\Lib\\site-packages\\datasets\\table.py:2247\u001b[39m, in \u001b[36mcast_table_to_schema\u001b[39m\u001b[34m(table, schema)\u001b[39m\n\u001b[32m   2241\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CastError(\n\u001b[32m   2242\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt cast\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(table.schema)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mbecause column names don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt match\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2243\u001b[39m         table_column_names=table.column_names,\n\u001b[32m   2244\u001b[39m         requested_column_names=\u001b[38;5;28mlist\u001b[39m(features),\n\u001b[32m   2245\u001b[39m     )\n\u001b[32m   2246\u001b[39m arrays = [\n\u001b[32m-> \u001b[39m\u001b[32m2247\u001b[39m     \u001b[43mcast_array_to_feature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtable_column_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2251\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, feature \u001b[38;5;129;01min\u001b[39;00m features.items()\n\u001b[32m   2252\u001b[39m ]\n\u001b[32m   2253\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pa.Table.from_arrays(arrays, schema=schema)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\coding\\CS_6303\\plot_data\\.venv\\Lib\\site-packages\\datasets\\table.py:1796\u001b[39m, in \u001b[36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[39m\u001b[34m(array, *args, **kwargs)\u001b[39m\n\u001b[32m   1795\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, pa.ChunkedArray):\n\u001b[32m-> \u001b[39m\u001b[32m1796\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pa.chunked_array([\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m array.chunks])\n\u001b[32m   1797\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\coding\\CS_6303\\plot_data\\.venv\\Lib\\site-packages\\datasets\\table.py:2109\u001b[39m, in \u001b[36mcast_array_to_feature\u001b[39m\u001b[34m(array, feature, allow_primitive_to_str, allow_decimal_to_str)\u001b[39m\n\u001b[32m   2103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m array_cast(\n\u001b[32m   2104\u001b[39m         array,\n\u001b[32m   2105\u001b[39m         feature(),\n\u001b[32m   2106\u001b[39m         allow_primitive_to_str=allow_primitive_to_str,\n\u001b[32m   2107\u001b[39m         allow_decimal_to_str=allow_decimal_to_str,\n\u001b[32m   2108\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2109\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt cast array of type\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(array.type)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(feature)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Couldn't cast array of type\nstruct<Depends on the city and the university: int64, idk: int64, no-answer: int64, not-applicable: int64>\nto\n{'idk': Value(dtype='int64', id=None), \"it's named differently\": Value(dtype='int64', id=None), 'no-answer': Value(dtype='int64', id=None), 'not-applicable': Value(dtype='int64', id=None)}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDatasetGenerationError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m ds = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnayeon212/BLEnD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mannotations\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\coding\\CS_6303\\plot_data\\.venv\\Lib\\site-packages\\datasets\\load.py:2084\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2081\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance.as_streaming_dataset(split=split)\n\u001b[32m   2083\u001b[39m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2084\u001b[39m \u001b[43mbuilder_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2085\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2086\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2087\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2088\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2089\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2090\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2092\u001b[39m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[32m   2093\u001b[39m keep_in_memory = (\n\u001b[32m   2094\u001b[39m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance.info.dataset_size)\n\u001b[32m   2095\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\coding\\CS_6303\\plot_data\\.venv\\Lib\\site-packages\\datasets\\builder.py:925\u001b[39m, in \u001b[36mDatasetBuilder.download_and_prepare\u001b[39m\u001b[34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[39m\n\u001b[32m    923\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    924\u001b[39m     prepare_split_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_proc\u001b[39m\u001b[33m\"\u001b[39m] = num_proc\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[32m    932\u001b[39m \u001b[38;5;28mself\u001b[39m.info.dataset_size = \u001b[38;5;28msum\u001b[39m(split.num_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info.splits.values())\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\coding\\CS_6303\\plot_data\\.venv\\Lib\\site-packages\\datasets\\builder.py:1001\u001b[39m, in \u001b[36mDatasetBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[39m\n\u001b[32m    997\u001b[39m split_dict.add(split_generator.split_info)\n\u001b[32m    999\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1000\u001b[39m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1003\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m   1004\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot find data file. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1005\u001b[39m         + (\u001b[38;5;28mself\u001b[39m.manual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1006\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1007\u001b[39m         + \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[32m   1008\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\coding\\CS_6303\\plot_data\\.venv\\Lib\\site-packages\\datasets\\builder.py:1742\u001b[39m, in \u001b[36mArrowBasedBuilder._prepare_split\u001b[39m\u001b[34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[39m\n\u001b[32m   1740\u001b[39m job_id = \u001b[32m0\u001b[39m\n\u001b[32m   1741\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m1742\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_split_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1743\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_prepare_split_args\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1745\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1746\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\coding\\CS_6303\\plot_data\\.venv\\Lib\\site-packages\\datasets\\builder.py:1898\u001b[39m, in \u001b[36mArrowBasedBuilder._prepare_split_single\u001b[39m\u001b[34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[39m\n\u001b[32m   1896\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, DatasetGenerationError):\n\u001b[32m   1897\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[33m\"\u001b[39m\u001b[33mAn error occurred while generating the dataset\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1900\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\n",
      "\u001b[31mDatasetGenerationError\u001b[39m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"nayeon212/BLEnD\", \"annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bafde35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits: ['DZ', 'AS', 'AZ', 'CN', 'ET', 'GR', 'ID', 'IR', 'MX', 'KP', 'NG', 'KR', 'ES', 'GB', 'US', 'JB']\n",
      "\n",
      "AZ split size: 500 rows\n",
      "\n",
      "Column names: ['ID', 'Topic', 'Source', 'Question', 'Translation']\n",
      "\n",
      "First 3 rows from AZ split:\n",
      "\n",
      "--- Row 0 ---\n",
      "{'ID': 'Al-en-01', 'Topic': 'Food', 'Source': 'English (US)', 'Question': 'Azərbaycanda məktəbəqədər uşaqlar üçün tipik qəlyanaltı nədir?', 'Translation': 'What is a common snack for preschool kids in Azerbaijan?'}\n",
      "\n",
      "--- Row 1 ---\n",
      "{'ID': 'Al-en-02', 'Topic': 'Food', 'Source': 'English (US)', 'Question': 'Azərbaycanda pivə ilə yaxşı gedən populyar yemək nədir?', 'Translation': 'What is a popular food to go with beer in Azerbaijan?'}\n",
      "\n",
      "--- Row 2 ---\n",
      "{'ID': 'Al-en-04', 'Topic': 'Food', 'Source': 'English (US)', 'Question': 'Azərbaycanda ən populyar meyvə hansıdır?', 'Translation': 'What is the most popular fruit in Azerbaijan?'}\n"
     ]
    }
   ],
   "source": [
    "# Display first few rows of the dataset\n",
    "print(f\"Dataset splits: {list(ds.keys())}\")\n",
    "print(f\"\\nAZ split size: {len(ds['AZ'])} rows\")\n",
    "print(f\"\\nColumn names: {ds['AZ'].column_names}\")\n",
    "print(\"\\nFirst 3 rows from AZ split:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Row {i} ---\")\n",
    "    print(ds['AZ'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75c546c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689bab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 100231 samples...\n",
      "Processed 10000 samples...\n",
      "Processed 10000 samples...\n",
      "Processed 20000 samples...\n",
      "Processed 20000 samples...\n",
      "Processed 30000 samples...\n",
      "Processed 30000 samples...\n",
      "Processed 40000 samples...\n",
      "Processed 40000 samples...\n",
      "Processed 50000 samples...\n",
      "Processed 50000 samples...\n",
      "Processed 60000 samples...\n",
      "Processed 60000 samples...\n",
      "Processed 70000 samples...\n",
      "Processed 70000 samples...\n",
      "Processed 80000 samples...\n",
      "Processed 80000 samples...\n",
      "Processed 90000 samples...\n",
      "Processed 90000 samples...\n",
      "Processed 100000 samples...\n",
      "\n",
      "Results:\n",
      "Average input tokens (query): 10.06\n",
      "Average output tokens (answer): 136.87\n",
      "Average input words (query): 9.06\n",
      "Average output words (answer): 100.34\n",
      "Total samples processed: 100231\n",
      "Processed 100000 samples...\n",
      "\n",
      "Results:\n",
      "Average input tokens (query): 10.06\n",
      "Average output tokens (answer): 136.87\n",
      "Average input words (query): 9.06\n",
      "Average output words (answer): 100.34\n",
      "Total samples processed: 100231\n"
     ]
    }
   ],
   "source": [
    "# Calculate average input and output tokens across ALL splits\n",
    "encoding_name = \"o200k_base\"  # GPT-4o, 4.1, 5 encoding\n",
    "\n",
    "# Get all available splits\n",
    "all_splits = list(ds.keys())\n",
    "print(f\"Processing {len(all_splits)} splits: {all_splits}\\n\")\n",
    "\n",
    "# Overall totals across all splits\n",
    "overall_total_input_tokens = 0\n",
    "overall_total_output_tokens = 0\n",
    "overall_total_input_words = 0\n",
    "overall_total_output_words = 0\n",
    "overall_total_samples = 0\n",
    "\n",
    "# Store per-split results\n",
    "split_results = []\n",
    "\n",
    "for split in all_splits:\n",
    "    print(f\"Processing split '{split}'...\")\n",
    "    \n",
    "    sample_size = len(ds[split])\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "    total_input_words = 0\n",
    "    total_output_words = 0\n",
    "    \n",
    "    for i in range(sample_size):\n",
    "        input_text = ds[split][i]['query']\n",
    "        output_text = ds[split][i]['answer']\n",
    "        \n",
    "        total_input_tokens += num_tokens_from_string(input_text, encoding_name)\n",
    "        total_output_tokens += num_tokens_from_string(output_text, encoding_name)\n",
    "        total_input_words += len(input_text.split())\n",
    "        total_output_words += len(output_text.split())\n",
    "        \n",
    "        if (i + 1) % 5000 == 0:\n",
    "            print(f\"  Processed {i + 1}/{sample_size} samples...\")\n",
    "    \n",
    "    # Calculate averages for this split\n",
    "    avg_input_tokens = total_input_tokens / sample_size if sample_size > 0 else 0\n",
    "    avg_output_tokens = total_output_tokens / sample_size if sample_size > 0 else 0\n",
    "    avg_input_words = total_input_words / sample_size if sample_size > 0 else 0\n",
    "    avg_output_words = total_output_words / sample_size if sample_size > 0 else 0\n",
    "    \n",
    "    split_results.append({\n",
    "        'split': split,\n",
    "        'samples': sample_size,\n",
    "        'avg_query_tokens': avg_input_tokens,\n",
    "        'avg_answer_tokens': avg_output_tokens,\n",
    "        'avg_query_words': avg_input_words,\n",
    "        'avg_answer_words': avg_output_words\n",
    "    })\n",
    "    \n",
    "    # Add to overall totals\n",
    "    overall_total_input_tokens += total_input_tokens\n",
    "    overall_total_output_tokens += total_output_tokens\n",
    "    overall_total_input_words += total_input_words\n",
    "    overall_total_output_words += total_output_words\n",
    "    overall_total_samples += sample_size\n",
    "    \n",
    "    print(f\"  Done: {sample_size} samples, Avg query tokens: {avg_input_tokens:.2f}, Avg answer tokens: {avg_output_tokens:.2f}\\n\")\n",
    "\n",
    "# Calculate overall averages\n",
    "overall_avg_input_tokens = overall_total_input_tokens / overall_total_samples if overall_total_samples > 0 else 0\n",
    "overall_avg_output_tokens = overall_total_output_tokens / overall_total_samples if overall_total_samples > 0 else 0\n",
    "overall_avg_input_words = overall_total_input_words / overall_total_samples if overall_total_samples > 0 else 0\n",
    "overall_avg_output_words = overall_total_output_words / overall_total_samples if overall_total_samples > 0 else 0\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"OVERALL RESULTS ACROSS ALL SPLITS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total splits processed: {len(split_results)}\")\n",
    "print(f\"Total samples processed: {overall_total_samples}\")\n",
    "print(f\"\\nOverall average query tokens: {overall_avg_input_tokens:.2f}\")\n",
    "print(f\"Overall average answer tokens: {overall_avg_output_tokens:.2f}\")\n",
    "print(f\"Overall average query words: {overall_avg_input_words:.2f}\")\n",
    "print(f\"Overall average answer words: {overall_avg_output_words:.2f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Display per-split breakdown\n",
    "import pandas as pd\n",
    "df_splits = pd.DataFrame(split_results)\n",
    "print(\"\\nPer-split breakdown:\")\n",
    "print(df_splits.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
