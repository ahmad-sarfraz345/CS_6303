{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4de6984",
   "metadata": {},
   "source": [
    "# Assignment 1 Part 2 Instructions **[50 marks]**\n",
    "\n",
    "## Overview\n",
    "You are required to develop an **initial chatbot system** and then implement **improvement(s)** on it. This process will involve an incremental development approach, where each new version builds upon the previous one. The goal is to improve the system’s performance in terms of response time.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Develop the Base Chatbot Implementation (20 marks)\n",
    "\n",
    "### Requirements\n",
    "- You must **implement a basic chatbot system** from scratch. This will serve as your starting point for future improvements.\n",
    "- The system should include **history tracking** (i.e., keeping track of the conversation history for context).\n",
    "- The chatbot must be able to provide consistent and accurate responses based on the conversation history.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: Implement Prompt Caching (15 marks)\n",
    "\n",
    "### Requirements\n",
    "- After completing your base chatbot, you must improve the system by introducing **prompt caching**.\n",
    "- Implement a system to **cache** prompts and responses to reduce unnecessary computations and improve response times.\n",
    "- Ensure that the caching mechanism enhances performance without compromising the correctness of responses.\n",
    "- Prompt caching should improve **response time**.\n",
    "\n",
    "---\n",
    "\n",
    "# Part 3 (Bonus): Implement Smart History (5+5 marks)\n",
    "\n",
    "### Requirements\n",
    "- Implement intelligent history tracking that **selectively stores relevant conversation snippets** rather than storing the entire history. \n",
    "- Focus on retaining only the most important or contextually relevant parts of the conversation.\n",
    "- Ensure that the system efficiently manages memory by not retaining unnecessary information.\n",
    "\n",
    "### Notes:\n",
    "- This section is **bonus** and can help recover **marks lost in any section** of this assignment.\n",
    "- **Marks gained in Part 3** are **not transferable** to any other grading components.\n",
    "- **5 marks** will be awarded for including detailed documentation of this implementation in the **report**. \n",
    "  - You **will not receive marks for code** unless you also provide **comprehensive details** of this improvement in the report, along with your other implementations.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Criteria\n",
    "All parts of the assignment will be evaluated based on the following:\n",
    "- **Correctness of Responses**: The system must generate correct answers for all the test cases.\n",
    "- **Performance**: The second implementation (with prompt caching) should be faster than the base system.\n",
    "- **Performance (Bonus)**: Your implementation of smart history must effectively manage memory and improve the system's overall performance.\n",
    "- **Correctness (Bonus)**: The chatbot should still produce accurate and contextually relevant responses while selectively storing history.\n",
    "\n",
    "  \n",
    "### Testing:\n",
    "- **Each implementation** will be **run three times**.\n",
    "- The **average of the best two runs** will be used for grading.\n",
    "\n",
    "---\n",
    "\n",
    "## Report (Must be reproducible) (15 marks)\n",
    "\n",
    "- **Graphs**: Include graphs showing **response time** for the base implementation and the improved implementation (Bonus as well if applicable) (all three runs). The graphs should clearly illustrate the performance improvement after each change.\n",
    "- **Journal of Thought Process**: Provide a detailed explanation of:\n",
    "  - Your design decisions and how you implemented each part.\n",
    "  - The reasoning behind introducing prompt caching and how it impacts system performance.\n",
    "- **Testing Results**: Clearly report the results of your testing, including the average times and performance metrics from the three runs of all versions.\n",
    "- In your summary, explain **how each step** (base system and prompt caching) improves the performance over the previous version.\n",
    "\n",
    "---\n",
    "\n",
    "## Deliverables:\n",
    "1. **Code**: Submit the code for both the base implementation and the improved version with prompt caching.\n",
    "2. **Report**: Submit a detailed, reproducible report that includes: (Bonus as well if applicable)\n",
    "   - Graphs comparing the base and improved system performance. \n",
    "   - A journal explaining your design decisions and reasoning behind each improvement.\n",
    "   - Testing results (average response times, resource usage, and other relevant metrics).\n",
    "3. **Generated Files**: Include the generated files from the runs of your base system and improved system.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acbfe77",
   "metadata": {},
   "source": [
    "## Imports and API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76edaece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\coding\\CS_6303\\PA1\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "from langchain import PromptTemplate\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "368d855e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Copy env file from part 1\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cc8f07",
   "metadata": {},
   "source": [
    "### Allowed Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64431bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS\n",
    "\n",
    "embed_model = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model1 = \"ministral-3b-latest\"\n",
    "model2 = \"ministral-8b-latest\"\n",
    "model3 = \"mistral-small-latest\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7271dd6d",
   "metadata": {},
   "source": [
    "### Prompts\n",
    "You may have more than one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbea5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base prompt\n",
    "prompt1 = PromptTemplate.from_template(\n",
    "    \"\"\"\\\n",
    "    You are a chatbot designed to answer questions only from within the conversation history, reply concisely and do not reply when no question is asked\"\n",
    "\n",
    "    Here is the conversation history:\n",
    "    {history}\n",
    "\n",
    "    Question: {query}  \n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "# prompt2 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1ceb56",
   "metadata": {},
   "source": [
    "## To-Do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a289392",
   "metadata": {},
   "source": [
    "### Part #1 <span style=\"color:green\">**[20 marks]**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa8045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bot_base:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # DO NOT remove any of the provided variables (You can add extra)\n",
    "        self.time = 0                                   # total response time\n",
    "        self.time_start = 0                             # start time for a query\n",
    "        self.time_end = 0                               # end time for a query\n",
    "        self.history = {}                               # var to store history (can be dict, list or any)\n",
    "        self.cac\n",
    "        \n",
    "        \n",
    "    # You are responsible for tracking the time taken for each query\n",
    "    def track_time(self, option):\n",
    "        if option == \"start\":\n",
    "            self.time_start = time.time()\n",
    "        elif option == \"end\":\n",
    "            self.time_end = time.time()\n",
    "            self.time += self.time_end - self.time_start\n",
    "\n",
    "    # TO-DO: This should return the reply from the chatbot as str\n",
    "    def generate(self, query, model=model3, prompt=prompt1):\n",
    "        self.track_time(\"start\")                                                \n",
    "        chat = ChatMistralAI(model=model, temperature=0, api_key=os.getenv(\"MISTRALAI_API_KEY\"))\n",
    "        if isinstance(self.history, dict):\n",
    "            history_str = \"\\n\".join([f\"User: {k}\\nBot: {v}\" for k, v in self.history.items()])\n",
    "        elif isinstance(self.history, list):\n",
    "            history_str = \"\\n\".join(self.history)\n",
    "        else:\n",
    "            history_str = str(self.history)\n",
    "        formatted_prompt = prompt.format(history=history_str, query=query)\n",
    "        response = chat.invoke(formatted_prompt)\n",
    "        if isinstance(self.history, dict):\n",
    "            self.history[query] = response\n",
    "        elif isinstance(self.history, list):\n",
    "            self.history.append(f\"User: {query}\\nBot: {response}\")\n",
    "        else:\n",
    "            self.history = f\"{self.history}\\nUser: {query}\\nBot: {response}\"\n",
    "        self.track_time(\"end\")\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f739400",
   "metadata": {},
   "source": [
    "### Part #2 <span style=\"color:green\">**[15 marks]**</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c80fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reuse code from the last part\n",
    "# implement prompt caching\n",
    "class Bot_with_cache(Bot_base):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cache = {}                                 # var to store cache (can be dict, list or any)\n",
    "        \n",
    "    # TO-DO: This should return the reply from the chatbot as str\n",
    "    def generate(self, query, model=model3, prompt=prompt1):\n",
    "        if query in self.cache:\n",
    "            return self.cache[query]\n",
    "        else:\n",
    "            response = super().generate(query, model=model, prompt=prompt)\n",
    "            self.cache[query] = response\n",
    "            return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05865141",
   "metadata": {},
   "source": [
    "### Part #3 (Bonus) <span style=\"color:green\">**[5+5 marks]**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7cf65a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reuse code from the last part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f92ac02",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedf78ec",
   "metadata": {},
   "source": [
    "### Loading test queries and functions\n",
    "[Do Not Change]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ef5e6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"In the realm of Asgard, the tallest mountain is Odin, a colossal peak that rises higher than any other in the Nine Realms. Towering at an awe-inspiring 29,032 feet above the shining plains of Vanahei\n"
     ]
    }
   ],
   "source": [
    "queries = {}\n",
    "\n",
    "with open(\"queries.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line and \"=\" in line:\n",
    "            key, value = line.split(\"=\", 1)\n",
    "            queries[key.strip()] = value.strip()\n",
    "\n",
    "print(queries[\"query1\"][:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd8d824",
   "metadata": {},
   "source": [
    "### Example Run\n",
    "\n",
    "You have to run each version 3 times and submit seperate txt file for each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e71743e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response for query 1: content='Odin' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 430, 'total_tokens': 433, 'completion_tokens': 3}, 'model_name': 'ministral-3b-latest', 'model': 'ministral-3b-latest', 'finish_reason': 'stop'} id='run--d2f47ef8-6f19-4806-90c3-ef71a6a8680f-0' usage_metadata={'input_tokens': 430, 'output_tokens': 3, 'total_tokens': 433}\n",
      "Response for query 2: content='In the coastal city of Marindale, the HelioWeave is the city-spanning canopy of solar textile that supplies 70 percent of Marindale’s electricity while shading streets, markets, and rooftops.' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 1790, 'total_tokens': 1837, 'completion_tokens': 47}, 'model_name': 'ministral-3b-latest', 'model': 'ministral-3b-latest', 'finish_reason': 'stop'} id='run--8abcfb4a-3e2e-4ee1-a35c-5e46f2527df7-0' usage_metadata={'input_tokens': 1790, 'output_tokens': 47, 'total_tokens': 1837}\n",
      "Response for query 3: content='The Ninefold Archive consists of nine underground vaults now housed at the Isola Museum of Origins.' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 3256, 'total_tokens': 3277, 'completion_tokens': 21}, 'model_name': 'ministral-3b-latest', 'model': 'ministral-3b-latest', 'finish_reason': 'stop'} id='run--6dcd4874-f6ea-4107-b630-56e4a377c196-0' usage_metadata={'input_tokens': 3256, 'output_tokens': 21, 'total_tokens': 3277}\n",
      "Response for query 4: content='Sleipnir' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 3694, 'total_tokens': 3700, 'completion_tokens': 6}, 'model_name': 'ministral-3b-latest', 'model': 'ministral-3b-latest', 'finish_reason': 'stop'} id='run--aea72cee-8db2-469d-852a-854054c72baa-0' usage_metadata={'input_tokens': 3694, 'output_tokens': 6, 'total_tokens': 3700}\n",
      "Response for query 5: content='70 percent' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 3876, 'total_tokens': 3880, 'completion_tokens': 4}, 'model_name': 'ministral-3b-latest', 'model': 'ministral-3b-latest', 'finish_reason': 'stop'} id='run--6155da2b-108b-4aa8-86f6-c44f2feda4e8-0' usage_metadata={'input_tokens': 3876, 'output_tokens': 4, 'total_tokens': 3880}\n",
      "Response for query 6: content='Alderwick' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 4369, 'total_tokens': 4373, 'completion_tokens': 4}, 'model_name': 'ministral-3b-latest', 'model': 'ministral-3b-latest', 'finish_reason': 'stop'} id='run--1aa70e18-0882-49b2-8cf7-cca18ea687a1-0' usage_metadata={'input_tokens': 4369, 'output_tokens': 4, 'total_tokens': 4373}\n",
      "Response for query 7: content='Traffic injuries inside the Green Ring district decreased by 42 percent in the first year.' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 4773, 'total_tokens': 4793, 'completion_tokens': 20}, 'model_name': 'ministral-3b-latest', 'model': 'ministral-3b-latest', 'finish_reason': 'stop'} id='run--15926920-3cee-461f-b770-20415fb72277-0' usage_metadata={'input_tokens': 4773, 'output_tokens': 20, 'total_tokens': 4793}\n",
      "Response for query 8: content='Odin' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 5175, 'total_tokens': 5178, 'completion_tokens': 3}, 'model_name': 'ministral-3b-latest', 'model': 'ministral-3b-latest', 'finish_reason': 'stop'} id='run--9679bb84-88dc-4e98-99db-0a522e61309a-0' usage_metadata={'input_tokens': 5175, 'output_tokens': 3, 'total_tokens': 5178}\n",
      "Response for query 9: content='Nine vaults.' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 5349, 'total_tokens': 5354, 'completion_tokens': 5}, 'model_name': 'ministral-3b-latest', 'model': 'ministral-3b-latest', 'finish_reason': 'stop'} id='run--ed4dcce5-cefa-42bf-87ee-8b04a95a09b2-0' usage_metadata={'input_tokens': 5349, 'output_tokens': 5, 'total_tokens': 5354}\n",
      "Response for query 10: content='Gungnir' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 5745, 'total_tokens': 5750, 'completion_tokens': 5}, 'model_name': 'ministral-3b-latest', 'model': 'ministral-3b-latest', 'finish_reason': 'stop'} id='run--1807209d-e4d4-47c9-9425-9db63eaaa1b3-0' usage_metadata={'input_tokens': 5745, 'output_tokens': 5, 'total_tokens': 5750}\n",
      "Response for query 11: content='Aethra' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 6110, 'total_tokens': 6114, 'completion_tokens': 4}, 'model_name': 'ministral-3b-latest', 'model': 'ministral-3b-latest', 'finish_reason': 'stop'} id='run--c200d03a-d1ec-40ec-8398-baf8643b7005-0' usage_metadata={'input_tokens': 6110, 'output_tokens': 4, 'total_tokens': 6114}\n",
      "Response for query 12: content='Eldros' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 6478, 'total_tokens': 6482, 'completion_tokens': 4}, 'model_name': 'ministral-3b-latest', 'model': 'ministral-3b-latest', 'finish_reason': 'stop'} id='run--58550988-eed1-4edc-81b9-4e36960a573e-0' usage_metadata={'input_tokens': 6478, 'output_tokens': 4, 'total_tokens': 6482}\n",
      "Response for query 13: content='Marindale' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 6657, 'total_tokens': 6661, 'completion_tokens': 4}, 'model_name': 'ministral-3b-latest', 'model': 'ministral-3b-latest', 'finish_reason': 'stop'} id='run--533ad559-68c5-47c9-80ad-5a4c0442a6d0-0' usage_metadata={'input_tokens': 6657, 'output_tokens': 4, 'total_tokens': 6661}\n",
      "Response for query 14: content='The true discovery site of the Ninefold Archive is Dunelith.' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 7010, 'total_tokens': 7025, 'completion_tokens': 15}, 'model_name': 'ministral-3b-latest', 'model': 'ministral-3b-latest', 'finish_reason': 'stop'} id='run--42b8bc35-1748-438a-8188-281b9b3912e5-0' usage_metadata={'input_tokens': 7010, 'output_tokens': 15, 'total_tokens': 7025}\n",
      "Response for query 15: content='The tallest tower in the Nine Realms is Hlidskjalf, and the mightiest forge is Surtr’s Crucible.' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 7199, 'total_tokens': 7228, 'completion_tokens': 29}, 'model_name': 'ministral-3b-latest', 'model': 'ministral-3b-latest', 'finish_reason': 'stop'} id='run--615ebeb0-73a3-4cc8-b2a7-e751931375f4-0' usage_metadata={'input_tokens': 7199, 'output_tokens': 29, 'total_tokens': 7228}\n",
      "Response for query 16: content='The mightiest forge in the Nine Realms is Surtr’s Crucible, and the tallest tower in the Nine Realms is Hlidskjalf.' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 7398, 'total_tokens': 7432, 'completion_tokens': 34}, 'model_name': 'ministral-3b-latest', 'model': 'ministral-3b-latest', 'finish_reason': 'stop'} id='run--75a2931d-ad46-45ef-a61a-92c6e1d0bac7-0' usage_metadata={'input_tokens': 7398, 'output_tokens': 34, 'total_tokens': 7432}\n",
      "Response for query 17: content='The Isola Museum of Origins' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 7603, 'total_tokens': 7610, 'completion_tokens': 7}, 'model_name': 'ministral-3b-latest', 'model': 'ministral-3b-latest', 'finish_reason': 'stop'} id='run--dcf9806e-72a2-469c-bf1d-2066fe2108cc-0' usage_metadata={'input_tokens': 7603, 'output_tokens': 7, 'total_tokens': 7610}\n",
      "Response for query 18: content='Marindale' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 7784, 'total_tokens': 7788, 'completion_tokens': 4}, 'model_name': 'ministral-3b-latest', 'model': 'ministral-3b-latest', 'finish_reason': 'stop'} id='run--9fbb9b92-f5e0-4d42-9b17-7614d07857dd-0' usage_metadata={'input_tokens': 7784, 'output_tokens': 4, 'total_tokens': 7788}\n",
      "Response for query 19: content='The true discovery site of the Ninefold Archive is Dunelith.' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 7780, 'total_tokens': 7795, 'completion_tokens': 15}, 'model_name': 'ministral-3b-latest', 'model': 'ministral-3b-latest', 'finish_reason': 'stop'} id='run--19887f7e-3e4c-4538-936a-f9860e68a34b-0' usage_metadata={'input_tokens': 7780, 'output_tokens': 15, 'total_tokens': 7795}\n",
      "Response for query 20: content='The Isola Museum of Origins' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 7970, 'total_tokens': 7977, 'completion_tokens': 7}, 'model_name': 'ministral-3b-latest', 'model': 'ministral-3b-latest', 'finish_reason': 'stop'} id='run--9343d788-c059-4448-a60c-50244784c22b-0' usage_metadata={'input_tokens': 7970, 'output_tokens': 7, 'total_tokens': 7977}\n",
      "Total time taken: 9.02719235420227 seconds\n"
     ]
    }
   ],
   "source": [
    "base= Bot_base()\n",
    "\n",
    "\n",
    "with open(\"base.txt\", \"w\") as f:\n",
    "    for i, query_num in enumerate(queries, start=1):\n",
    "        try:\n",
    "            query = queries[query_num]\n",
    "            response = base.generate(query=query, prompt=prompt1, model=model1)\n",
    "\n",
    "            print(f\"Response for query {i}: {response}\")\n",
    "            f.write(f\"Query {i}: {query}\\nResponse: {response}\\n\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query {i}: {e}\")\n",
    "            break\n",
    "        \n",
    "print(f\"Total time taken: {base.time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af93277",
   "metadata": {},
   "source": [
    "### Report <span style=\"color:green\">**[15 marks]**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b58b9a6",
   "metadata": {},
   "source": [
    "### Report Submission Guidelines\n",
    "\n",
    "Along with your improved implementations, you must submit a **report in PDF format** that documents your work. This report is a crucial part of the assignment and will be graded for clarity, completeness, and reproducibility.  \n",
    "\n",
    "Your report must include:  \n",
    "1. **Findings**: A clear summary of results from each implementation. (table form)  \n",
    "2. **Mechanisms Used**: A detailed explanation of the methods and architectural changes applied at each step.  \n",
    "3. **Thought Process**: A journal-style reflection describing your reasoning for applying each change.  \n",
    "4. **Graphs**: Visualizations showing the changes in **response time** and **API cost** after each implementation.  \n",
    "\n",
    "> ⚠️ **Important**: While the PDF should contain the graphs, the **code used to generate these graphs must be included in the notebook cells below this section**. This ensures that your results are reproducible.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b4178bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for graphs and report goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217f4dbb",
   "metadata": {},
   "source": [
    "## End of Part 2\n",
    "\n",
    "You must submit:  \n",
    "- The **current notebook file** (`.ipynb`).  \n",
    "- Its **Python conversion** (`.py` file).  \n",
    "- The **Report** (`.pdf`).\n",
    "- Run **files** (`.txt`).\n",
    "\n",
    "All files should be placed inside a folder named \"RollNumber_PA1\". This folder must also include your **Part 1 files**, and the entire folder should be **zipped and submitted**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
