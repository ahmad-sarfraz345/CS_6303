{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Part 1 Instructions **[50 marks]**\n",
    "\n",
    "## Overview\n",
    "You are required to develop a **Retrieval-Augmented Generation (RAG) system** from scratch. This includes implementing embeddings, storing them in a vector database, building a retriever, and chaining it with an LLM. You will then **experiment with retrieval parameters** and analyze their effects on system accuracy. A bonus section will allow you to refine the system with **prompt engineering**.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Build the RAG Chain (20 marks)\n",
    "\n",
    "### Requirements\n",
    "- Implement embeddings and store them in a **vector database**.  \n",
    "- Implement a retriever and build the **RAG chain** that connects it with an LLM.  \n",
    "- Ensure the pipeline can retrieve relevant chunks and generate answers.  \n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: Chaining for Extended Context (10 marks)\n",
    "\n",
    "### Requirements\n",
    "- Implement **chaining** that allows the LLM to go beyond retrieved context.  \n",
    "- Demonstrate how chaining enables multi-step reasoning or broader context handling.  \n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: Parameter Testing & Report (20 marks)\n",
    "\n",
    "### Requirements\n",
    "- Use the provided dataset of **20 questions** (10 with one-word answers, 10 with no answer).  \n",
    "- Explore different values for:  \n",
    "  - **Chunk size**  \n",
    "  - **Chunk overlap**  \n",
    "  - **Top-K retrieval**  \n",
    "  - **Similarity cutoff threshold**  \n",
    "- Record and analyze how parameter choices affect accuracy.  \n",
    "- Submit a **report** summarizing:  \n",
    "  - The results of your experiments.  \n",
    "  - Explanations of why certain parameters improved or degraded performance.  \n",
    "\n",
    "---\n",
    "\n",
    "## Bonus: Prompt Engineering (5+5 marks)\n",
    "\n",
    "### Requirements\n",
    "- Refine your system using **prompt engineering** so that:  \n",
    "  - For answerable questions → the LLM provides the correct answer.  \n",
    "  - For unanswerable questions → the LLM responds with *“I don’t know”*.  \n",
    "\n",
    "### Notes:\n",
    "- Bonus marks can be used to recover deductions from earlier parts.  \n",
    "- Creativity and clarity in prompt design will be rewarded.  \n",
    "- Do **not** use GPT or other AI tools to generate code. Instead, consult the documentation — learning to use these tools independently is essential for your course projects.  \n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Criteria\n",
    "- **Correctness**: The RAG chain should work end-to-end.  \n",
    "- **Implementation**: Proper embeddings, vector DB, retriever, and chaining.  \n",
    "- **Analysis**: Depth of reasoning in your parameter exploration report.  \n",
    "- **Bonus (Optional)**: Effective prompt engineering to balance correctness and abstention.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "First, we install the required packages:\n",
    "\n",
    "- **LangChain** and **LangChain Community**: provide loaders, splitters, retrievers, etc.  \n",
    "- **LangChain HuggingFace**: for embedding models.  \n",
    "- **LangChain Pinecone** and **pinecone-client**: for vector database storage.  \n",
    "- **python-dotenv**: for managing API keys securely.  \n",
    "- **Streamlit**: (optional) for building a user interface.  \n",
    "- **MistralAI client**: for the language model we’ll use.\n",
    "\n",
    "This ensures our environment has all dependencies for building a RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-community\n",
    "%pip install langchain-huggingface pypdf\n",
    "%pip install langchain-pinecone pinecone-client\n",
    "%pip install python-dotenv streamlit\n",
    "%pip install -qU langchain-mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\coding\\uni\\CS_6303\\PA1\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\coding\\uni\\CS_6303\\PA1\\.venv\\Lib\\site-packages\\langchain_pinecone\\__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from uuid import uuid4\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter as RCTS\n",
    "from langchain_mistralai import ChatMistralAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: RAG Chatbot  <span style=\"color:green\">**[20 marks]**</span>\n",
    "\n",
    "For guidance, you may refer to the following resources:  \n",
    "- [RAG Overview](https://python.langchain.com/docs/tutorials/rag/#overview)  \n",
    "- [How-to Guides](https://python.langchain.com/docs/how_to/)  \n",
    "- [Concepts](https://python.langchain.com/docs/concepts/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting API Keys\n",
    "\n",
    "We need API keys for external services:\n",
    "- [HuggingFace](https://huggingface.co/settings/tokens) (for embeddings)\n",
    "- [Pinecone](https://app.pinecone.io/) (for vector storage)\n",
    "- [MistralAI](https://console.mistral.ai/api-keys) (for the chat model)\n",
    "\n",
    "Here we define them as variables. Later we’ll store them securely using environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You only need to run this cell once to create the .env file with your API keys.\n",
    "\n",
    "HUGGINGFACE_API_KEY = \"placeholder, .env created\"  # Replace with your Hugging Face API key\n",
    "PINECONE_API_KEY = \"placeholder, .env created\"        # Replace with your Pinecone API key\n",
    "MISTRALAI_API_KEY = \"placeholder, .env created\"       # Replace with your Mistral AI API key\n",
    "\n",
    "env_content = f\"\"\"\n",
    "HUGGINGFACE_API_KEY={HUGGINGFACE_API_KEY}\n",
    "PINECONE_API_KEY={PINECONE_API_KEY}\n",
    "MISTRALAI_API_KEY={MISTRALAI_API_KEY}\n",
    "\"\"\"\n",
    "\n",
    "with open(\".env\", \"w\") as file:\n",
    "    file.write(env_content)\n",
    "\n",
    "print(\"Environment variables are saved to .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Environment Variables\n",
    "\n",
    "We use `dotenv` to manage keys.  \n",
    "Instead of hardcoding secrets, we load them from a `.env` file.  \n",
    "This keeps our notebook secure and avoids accidentally sharing keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Splitting Documents\n",
    "\n",
    "Our knowledge source is a **PDF document** (the Undergraduate Student Handbook).  \n",
    "Steps:\n",
    "1. Load the PDF using `PyPDFLoader`.  \n",
    "2. Split the text into smaller chunks using `RecursiveCharacterTextSplitter`.  \n",
    "\n",
    "Why split?  \n",
    "- Large text doesn’t fit into the LLM’s context window.  \n",
    "- Smaller chunks make retrieval more accurate during question-answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Undergraduate Student Handbook 2021-2022.pdf'\n",
    "loader = PyPDFLoader(file_path)\n",
    "documents = loader.load()\n",
    "\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 4\n",
    "\n",
    "text_splitter = RCTS(chunk_size=chunk_size, chunk_overlap=chunk_overlap, add_start_index=True)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Initialize Pinecone and Create a Vector Store\n",
    "\n",
    "Your task is to **set up Pinecone** as the vector database for this RAG system.  \n",
    "\n",
    "#### Requirements:  \n",
    "- If the index with the given name already exists, reuse it. Otherwise, **create a new index**.  \n",
    "- Use the following configuration for the index:\n",
    "  - **Dimension:** 768  \n",
    "  - **Metric:** cosine similarity  \n",
    "  - **Cloud:** AWS, region `us-east-1`  \n",
    "- Initialize the **embedding model** (`HuggingFaceEmbeddings`).  \n",
    "- Return a **`PineconeVectorStore`** object.  \n",
    "\n",
    "> 💡 *Hint:* Make sure you return the vector store at the end of the function so it can be used later in your RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_pinecone(pc, index_name):\n",
    "    # TODO: Check if index_name already exists, otherwise create it\n",
    "    # - Use dimension=768\n",
    "    # - Metric=\"cosine\"\n",
    "    # - Cloud=\"aws\"\n",
    "    # - Region=\"us-east-1\"\n",
    "\n",
    "    existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "    if index_name not in existing_indexes:\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=768,\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "        )\n",
    "\n",
    "    # TODO: Initialize embeddings model (default)\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings()  # Use CPU for embeddings\n",
    "\n",
    "    # TODO: Load the index and initialize the PineconeVectorStore\n",
    "    index = pc.Index(index_name)\n",
    "    vector_store = PineconeVectorStore(index=index, embedding=embeddings) \n",
    "\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Add Documents to Pinecone\n",
    "\n",
    "Your task is to **add documents to the Pinecone vector store**.  \n",
    "\n",
    "#### Requirements:\n",
    "- Each document must be assigned a **unique ID** before insertion.  \n",
    "- Use `uuid4()` to generate IDs for all documents.  \n",
    "- Add the provided list of texts to the given **vector store**.  \n",
    "- Ensure the function returns nothing but successfully inserts the documents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_documents_to_pinecone(documents, vector_store):\n",
    "    # TODO: Generate a unique ID for each text using uuid4\n",
    "    uuids = [str(uuid4()) for _ in range(len(documents))]  \n",
    "\n",
    "    # TODO: Add texts and their IDs to the vector store\n",
    "    vector_store.add_texts(texts=[doc.page_content for doc in documents], ids=uuids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Create a Retriever from Vector Store\n",
    "\n",
    "Your task is to **create a retriever** that will query the vector store for the most relevant documents.  \n",
    "\n",
    "#### Requirements:\n",
    "- Convert the `vector_store` into a retriever using `.as_retriever()`.  \n",
    "- Use **similarity with score threshold** as the search type.  \n",
    "- The retriever should take two parameters:  \n",
    "  - **top_k** → maximum number of documents to retrieve.  \n",
    "  - **score_threshold** → minimum similarity score required for retrieval.  \n",
    "- Return the retriever object.  \n",
    "\n",
    "> 💡 *Hint:* Adjusting `top_k` and `score_threshold` will be important later when you experiment with retrieval performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever(vector_store, top_k=2, score_threshold=0.5):\n",
    "    # TODO: Create a retriever using similarity with score threshold\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": top_k, \"score_threshold\": score_threshold})  \n",
    "\n",
    "    return retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a Simple Similarity Search\n",
    "\n",
    "Now that you have implemented the helper functions, it’s time to test the full retrieval pipeline.\n",
    "\n",
    "#### Steps:\n",
    "1. Initialize Pinecone using your API key (`PINECONE_API_KEY`) and then a vector store.  \n",
    "2. Add handbook documents to the vector store using `add_documents_to_pinecone()`.  \n",
    "3. Run a **similarity search** against the vector store with a query (e.g., *\"Grading Policy\"*).  \n",
    "4. Retrieve the top `k` chunks and print out their content and metadata.  \n",
    "\n",
    "> 💡 *Hint:* This step ensures that your embeddings and vector store are working correctly before moving on to building the RAG chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ForbiddenException",
     "evalue": "(403)\nReason: Forbidden\nHTTP response headers: HTTPHeaderDict({'content-type': 'text/plain; charset=utf-8', 'access-control-allow-origin': '*', 'vary': 'origin,access-control-request-method,access-control-request-headers', 'access-control-expose-headers': '*', 'x-pinecone-api-version': '2025-04', 'x-cloud-trace-context': 'fa1795f983fdc906b46da4cdcedf4b1a', 'date': 'Sun, 05 Oct 2025 11:17:21 GMT', 'server': 'Google Frontend', 'Content-Length': '257', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\nHTTP response body: {\"error\":{\"code\":\"FORBIDDEN\",\"message\":\"Request failed. You've reached the max serverless indexes allowed in project Default (5). Use namespaces to partition your data into logical groups, or upgrade your plan to add more serverless indexes.\"},\"status\":403}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mForbiddenException\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Step 1: Initialize Pinecone and create/reuse vector store\u001b[39;00m\n\u001b[32m      2\u001b[39m pc = Pinecone(api_key=os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mPINECONE_API_KEY\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m vector_store = \u001b[43minitialize_pinecone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhandbook\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Step 2: we will use the previously created `docs` variable\u001b[39;00m\n\u001b[32m      6\u001b[39m add_documents_to_pinecone(docs, vector_store)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36minitialize_pinecone\u001b[39m\u001b[34m(pc, index_name)\u001b[39m\n\u001b[32m      8\u001b[39m existing_indexes = [index_info[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m index_info \u001b[38;5;129;01min\u001b[39;00m pc.list_indexes()]\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m existing_indexes:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mpc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdimension\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m768\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcosine\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mServerlessSpec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcloud\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maws\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mus-east-1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# TODO: Initialize embeddings model (default)\u001b[39;00m\n\u001b[32m     20\u001b[39m embeddings = HuggingFaceEmbeddings()  \u001b[38;5;66;03m# Use CPU for embeddings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\coding\\uni\\CS_6303\\PA1\\.venv\\Lib\\site-packages\\pinecone\\pinecone.py:334\u001b[39m, in \u001b[36mPinecone.create_index\u001b[39m\u001b[34m(self, name, spec, dimension, metric, timeout, deletion_protection, vector_type, tags)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_index\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    325\u001b[39m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    332\u001b[39m     tags: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    333\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mIndexModel\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdimension\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdimension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeletion_protection\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeletion_protection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvector_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvector_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\coding\\uni\\CS_6303\\PA1\\.venv\\Lib\\site-packages\\pinecone\\utils\\require_kwargs.py:14\u001b[39m, in \u001b[36mrequire_kwargs.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m      9\u001b[39m     param_names = \u001b[38;5;28mlist\u001b[39m(inspect.signature(func).parameters.keys())[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# Skip self\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m     11\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m() requires keyword arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease use \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=value\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mname\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mparam_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\coding\\uni\\CS_6303\\PA1\\.venv\\Lib\\site-packages\\pinecone\\db_control\\resources\\sync\\index.py:81\u001b[39m, in \u001b[36mIndexResource.create\u001b[39m\u001b[34m(self, name, spec, dimension, metric, timeout, deletion_protection, vector_type, tags)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;129m@require_kwargs\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m     tags: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     71\u001b[39m ) -> IndexModel:\n\u001b[32m     72\u001b[39m     req = PineconeDBControlRequestFactory.create_index_request(\n\u001b[32m     73\u001b[39m         name=name,\n\u001b[32m     74\u001b[39m         spec=spec,\n\u001b[32m   (...)\u001b[39m\u001b[32m     79\u001b[39m         tags=tags,\n\u001b[32m     80\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_index_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_index_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout == -\u001b[32m1\u001b[39m:\n\u001b[32m     84\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m IndexModel(resp)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\coding\\uni\\CS_6303\\PA1\\.venv\\Lib\\site-packages\\pinecone\\openapi_support\\endpoint.py:102\u001b[39m, in \u001b[36mEndpoint.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m     92\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"This method is invoked when endpoints are called\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m     94\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    100\u001b[39m \n\u001b[32m    101\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\coding\\uni\\CS_6303\\PA1\\.venv\\Lib\\site-packages\\pinecone\\core\\openapi\\db_control\\api\\manage_indexes_api.py:322\u001b[39m, in \u001b[36mManageIndexesApi.__init__.<locals>.__create_index\u001b[39m\u001b[34m(self, create_index_request, **kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m kwargs = \u001b[38;5;28mself\u001b[39m._process_openapi_kwargs(kwargs)\n\u001b[32m    321\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mcreate_index_request\u001b[39m\u001b[33m\"\u001b[39m] = create_index_request\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_with_http_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\coding\\uni\\CS_6303\\PA1\\.venv\\Lib\\site-packages\\pinecone\\openapi_support\\endpoint.py:134\u001b[39m, in \u001b[36mEndpoint.call_with_http_info\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    124\u001b[39m params = EndpointUtils.gather_params(\n\u001b[32m    125\u001b[39m     attribute_map=\u001b[38;5;28mself\u001b[39m.attribute_map,\n\u001b[32m    126\u001b[39m     location_map=\u001b[38;5;28mself\u001b[39m.location_map,\n\u001b[32m   (...)\u001b[39m\u001b[32m    129\u001b[39m     kwargs=kwargs,\n\u001b[32m    130\u001b[39m )\n\u001b[32m    132\u001b[39m HeaderUtil.prepare_headers(headers_map=\u001b[38;5;28mself\u001b[39m.headers_map, params=params)\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mendpoint_path\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttp_method\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheader_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mheader\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbody\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfile\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43masync_req\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43masync_req\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43masync_threadpool_executor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43masync_threadpool_executor\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_check_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_check_return_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_return_http_data_only\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_preload_content\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_request_timeout\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_host\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcollection_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\coding\\uni\\CS_6303\\PA1\\.venv\\Lib\\site-packages\\pinecone\\openapi_support\\api_client.py:306\u001b[39m, in \u001b[36mApiClient.call_api\u001b[39m\u001b[34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, async_threadpool_executor, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.threadpool_executor.submit(\n\u001b[32m    286\u001b[39m         \u001b[38;5;28mself\u001b[39m.__call_api,\n\u001b[32m    287\u001b[39m         resource_path,\n\u001b[32m   (...)\u001b[39m\u001b[32m    302\u001b[39m         _check_type,\n\u001b[32m    303\u001b[39m     )\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m async_req:\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__call_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresource_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_check_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pool.apply_async(\n\u001b[32m    326\u001b[39m     \u001b[38;5;28mself\u001b[39m.__call_api,\n\u001b[32m    327\u001b[39m     (\n\u001b[32m   (...)\u001b[39m\u001b[32m    344\u001b[39m     ),\n\u001b[32m    345\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\coding\\uni\\CS_6303\\PA1\\.venv\\Lib\\site-packages\\pinecone\\openapi_support\\api_client.py:182\u001b[39m, in \u001b[36mApiClient.__call_api\u001b[39m\u001b[34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m PineconeApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m     e.body = e.body.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    184\u001b[39m \u001b[38;5;28mself\u001b[39m.last_response = response_data\n\u001b[32m    186\u001b[39m return_data = response_data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\coding\\uni\\CS_6303\\PA1\\.venv\\Lib\\site-packages\\pinecone\\openapi_support\\api_client.py:170\u001b[39m, in \u001b[36mApiClient.__call_api\u001b[39m\u001b[34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[39m\n\u001b[32m    161\u001b[39m url = build_request_url(\n\u001b[32m    162\u001b[39m     config=config,\n\u001b[32m    163\u001b[39m     processed_path_params=path_params_tuple,\n\u001b[32m    164\u001b[39m     resource_path=resource_path,\n\u001b[32m    165\u001b[39m     _host=_host,\n\u001b[32m    166\u001b[39m )\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# perform request and return response\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     response_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessed_query_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessed_post_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m PineconeApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m     e.body = e.body.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\coding\\uni\\CS_6303\\PA1\\.venv\\Lib\\site-packages\\pinecone\\openapi_support\\api_client.py:386\u001b[39m, in \u001b[36mApiClient.request\u001b[39m\u001b[34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[39m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rest_client.OPTIONS(\n\u001b[32m    377\u001b[39m         url,\n\u001b[32m    378\u001b[39m         query_params=query_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m         body=body,\n\u001b[32m    384\u001b[39m     )\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrest_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPOST\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mPUT\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    396\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rest_client.PUT(\n\u001b[32m    397\u001b[39m         url,\n\u001b[32m    398\u001b[39m         query_params=query_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    403\u001b[39m         body=body,\n\u001b[32m    404\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\coding\\uni\\CS_6303\\PA1\\.venv\\Lib\\site-packages\\pinecone\\openapi_support\\rest_utils.py:146\u001b[39m, in \u001b[36mRestClientInterface.POST\u001b[39m\u001b[34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mPOST\u001b[39m(\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    138\u001b[39m     url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    144\u001b[39m     _request_timeout=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    145\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\coding\\uni\\CS_6303\\PA1\\.venv\\Lib\\site-packages\\pinecone\\openapi_support\\rest_urllib3.py:267\u001b[39m, in \u001b[36mUrllib3RestClient.request\u001b[39m\u001b[34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[39m\n\u001b[32m    264\u001b[39m     \u001b[38;5;66;03m# log response body\u001b[39;00m\n\u001b[32m    265\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mresponse body: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, r.data)\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mraise_exceptions_or_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\coding\\uni\\CS_6303\\PA1\\.venv\\Lib\\site-packages\\pinecone\\openapi_support\\rest_utils.py:41\u001b[39m, in \u001b[36mraise_exceptions_or_return\u001b[39m\u001b[34m(r)\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UnauthorizedException(http_resp=r)\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m r.status == \u001b[32m403\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ForbiddenException(http_resp=r)\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m r.status == \u001b[32m404\u001b[39m:\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NotFoundException(http_resp=r)\n",
      "\u001b[31mForbiddenException\u001b[39m: (403)\nReason: Forbidden\nHTTP response headers: HTTPHeaderDict({'content-type': 'text/plain; charset=utf-8', 'access-control-allow-origin': '*', 'vary': 'origin,access-control-request-method,access-control-request-headers', 'access-control-expose-headers': '*', 'x-pinecone-api-version': '2025-04', 'x-cloud-trace-context': 'fa1795f983fdc906b46da4cdcedf4b1a', 'date': 'Sun, 05 Oct 2025 11:17:21 GMT', 'server': 'Google Frontend', 'Content-Length': '257', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\nHTTP response body: {\"error\":{\"code\":\"FORBIDDEN\",\"message\":\"Request failed. You've reached the max serverless indexes allowed in project Default (5). Use namespaces to partition your data into logical groups, or upgrade your plan to add more serverless indexes.\"},\"status\":403}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize Pinecone and create/reuse vector store\n",
    "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "vector_store = initialize_pinecone(pc, index_name=\"handbook\")\n",
    "\n",
    "# Step 2: we will use the previously created `docs` variable\n",
    "add_documents_to_pinecone(docs, vector_store)\n",
    "\n",
    "# Step 3: Initialize retriever\n",
    "k = 2\n",
    "threshold = 0.5\n",
    "retriever = create_retriever(vector_store, top_k=k, score_threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Step 4: Perform a similarity search with a query\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results = \u001b[43mretriever\u001b[49m.get_relevant_documents(\u001b[33m\"\u001b[39m\u001b[33mGrading Policy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Step 5: Print out the content and metadata of the retrieved results\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "\u001b[31mNameError\u001b[39m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 4: Perform a similarity search with a query\n",
    "results = retriever.get_relevant_documents(\"Grading Policy\")\n",
    "\n",
    "# Step 5: Print out the content and metadata of the retrieved results\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Chat Model (Mistral AI)\n",
    "\n",
    "We now set up the **LLM** that will generate answers.  \n",
    "Here we initialize the **[Mistral model](https://docs.mistral.ai/getting-started/models/models_overview/)** using our API key.  \n",
    "\n",
    "This model will take the retrieved handbook text + user question, and generate a clear answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = \"ministral-3b-latest\"\n",
    "llm = ChatMistralAI(\n",
    "    model=model,\n",
    "    temperature=0,\n",
    "    max_retries=1,\n",
    "    api_key=os.getenv(\"MISTRALAI_API_KEY\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Prompt Template\n",
    "\n",
    "Language models work best when guided with **clear instructions**.  \n",
    "Here we create a **PromptTemplate** that tells the chatbot exactly how to behave:\n",
    "\n",
    "- It is designed to answer questions **only about the LUMS Student Handbook**.  \n",
    "- It must use the **given context** (retrieved chunks from the handbook).  \n",
    "- If the answer is not in the context, it should respond with **“I don’t know”**.  \n",
    "- The template takes two inputs:  \n",
    "  1. **context** – text retrieved from the handbook.  \n",
    "  2. **question** – the student’s query.  \n",
    "\n",
    "This ensures the chatbot gives reliable answers and avoids making things up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the modified prompt template\n",
    "template = \"\"\"\n",
    "You are a chatbot designed to answer questions from LUMS students. LUMS is a university and you have access to the student handbook.\n",
    "Use following extract from the handbook to answer the question.\n",
    "If the context doesn't contain any relevant information to the question, then just say \"I don't know\".\n",
    "If you don't know the answer, then just say \"I don't know\".\n",
    "Do NOT make something up.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RAG Chain\n",
    "\n",
    "Now that you have a retriever & LLM, it’s time to connect both with a **prompt template** to form the RAG pipeline.\n",
    "\n",
    "### Requirements:\n",
    "- Take in three inputs:\n",
    "  - **retriever** → to fetch relevant context from the vector store.  \n",
    "  - **prompt** → the prompt template that structures inputs for the LLM.  \n",
    "  - **llm** → the language model used for generation.  \n",
    "- Chain the components together in the following order:\n",
    "  1. Retrieve documents (`retriever | format_docs`).  \n",
    "  2. Pass context and the user question into the **prompt**.  \n",
    "  3. Send the formatted prompt to the **LLM**.  \n",
    "  4. Parse the response into a clean string (`StrOutputParser`).  \n",
    "- Return the completed RAG chain object.  \n",
    "\n",
    "> 💡 *Hint:* This chain will be the foundation for answering user queries with retrieved context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):  # stitches together retrieved documents\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "def build_rag_chain(retriever, prompt, llm):\n",
    "    # TODO: Combine retriever, prompt, and llm into a chain\n",
    "    rag_chain = (\n",
    "        # 1. Retrieve docs and format them\n",
    "        {\"context\": retriever | format_docs,\n",
    "         \"question\": RunnablePassthrough()} # using runnable pass through to pass the question as is (from the .invoke)\n",
    "        # 2. Fill prompt with context + question\n",
    "        | prompt\n",
    "        # 3. Send to LLM\n",
    "        | llm\n",
    "        # 4. Parse clean string output\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return rag_chain\n",
    "\n",
    "rag_chain = build_rag_chain(retriever, prompt, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Test the RAG Chain\n",
    "\n",
    "Now that your RAG chain is built, it’s time to **run a query** and inspect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grading policy for LUMS is based on cumulative performance in defined instruments. The final grades are assigned as follows:\n",
      "\n",
      "- A+ (4.0)\n",
      "- A (4.0)\n",
      "- A- (3.7)\n",
      "- B+ (3.3)\n",
      "- B (3.0)\n",
      "- B- (2.7)\n",
      "- C+ (2.3)\n",
      "- Low Pass C (2.0)\n",
      "- Marginal Pass C- (1.7)\n",
      "- Unacceptable D (1.0)\n",
      "- Pass (P) (0.0)\n",
      "- Fail (F) (0.0)\n",
      "- Withdrawn (W) (0.0)\n",
      "- Incomplete (I) (0.0)\n",
      "- Transfer (T) (0.0)\n",
      "\n",
      "Grading at LUMS is based on relative performance, but for some courses, absolute grading is used. This information is mentioned in the course outline.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the grading policy for the university?\"\n",
    "result = rag_chain.invoke(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minors offered at SBASSE include Biology, Chemistry, Computer Science, Mathematics, and Physics.\n"
     ]
    }
   ],
   "source": [
    "question = \"What are some minors offered at SBASSE?\"\n",
    "result = rag_chain.invoke(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Testing its limitations\n",
    "\n",
    "General questions that are not related to the university would not answered due to the way we structured the prompt template. Try asking such questions below to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "question = \"What are some gift ideas for Mothers Day?\"\n",
    "result = rag_chain.invoke(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the fastest growing plant?\"\n",
    "result = rag_chain.invoke(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Dynamic Routing <span style=\"color:green\">**[10 marks]**</span>\n",
    "\n",
    "One of the key advantages of LangChain is the ability to create **non-deterministic chains**, where the output of one step determines the next.  \n",
    "\n",
    "In this section, you will:\n",
    "1. **Build a classifier chain** to decide whether a user question is about *education/academic policies* or *Other*. [5 marks]  \n",
    "2. **Build a general LLM chain** that handles non-policy queries. [5 marks]  \n",
    "3. **Implement routing logic** that uses the classifier output to decide whether to send the query to the **RAG chain** (for policy-related questions) or to the **general chain** (for everything else). [10 marks]  \n",
    "\n",
    "#### Requirements:\n",
    "- Use a **prompt template** for classification (must only return `\"education/academic policies\"` or `\"Other\"`).  \n",
    "- Build a **general chain** that simply responds directly to a user’s query.  \n",
    "- Implement a `route()` function that:  \n",
    "  - Sends questions about education/academic policies → `rag_chain`.  \n",
    "  - Sends all other questions → `general_chain`.  \n",
    "- Combine everything into a `full_chain` that takes a question and automatically routes it.  \n",
    "\n",
    "> 💡 *Hint:* Use `RunnableLambda` for your routing logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example prompts for classification and general QA\n",
    "\n",
    "classifier_template = \"\"\"Given the user question below, classify it as either being about `education/academic policies`, or `Other`.\n",
    "\n",
    "Do not respond with anything other than 'education/academic policies' or 'Other'.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Classification:\"\"\"\n",
    "\n",
    "general_template = \"\"\"Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the classifier chain\n",
    "classifier_prompt = PromptTemplate(template=classifier_template, input_variables=[\"question\"])\n",
    "classifier_chain = {\"question\": RunnablePassthrough()} | classifier_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "# 2. Define the general LLM chain\n",
    "general_prompt = PromptTemplate(template=general_template, input_variables=[\"question\"])\n",
    "general_chain = {\"question\": RunnablePassthrough()} | general_prompt | llm | StrOutputParser()\n",
    "\n",
    "# 3. Implement routing logic \n",
    "def route(info):\n",
    "    # TODO: Implement routing logic based on the topic\n",
    "    label = info[\"pred\"].strip().lower()\n",
    "    question = info[\"question\"][\"question\"]\n",
    "    if label == \"education/academic policies\":\n",
    "        return rag_chain.invoke(question)\n",
    "    else:\n",
    "        return general_chain.invoke({\"question\": question})\n",
    "\n",
    "# Combine into the full chain\n",
    "full_chain = {\"pred\": classifier_chain, \"question\": RunnablePassthrough()} | RunnableLambda(route)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Our Improved ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grading policy for LUMS is based on cumulative performance in defined instruments. The final grades are assigned as follows:\n",
      "\n",
      "- A+ (4.0)\n",
      "- A (4.0)\n",
      "- A- (3.7)\n",
      "- B+ (3.3)\n",
      "- B (3.0)\n",
      "- B- (2.7)\n",
      "- C+ (2.3)\n",
      "- Low Pass C (2.0)\n",
      "- Marginal Pass C- (1.7)\n",
      "- Unacceptable D (1.0)\n",
      "- Pass (P) (0.0)\n",
      "- Fail (F) (0.0)\n",
      "- Withdrawn (W) (0.0)\n",
      "- Incomplete (I) (0.0)\n",
      "- Transfer (T) (0.0)\n",
      "\n",
      "Grading at LUMS is based on relative performance, but for some courses, absolute grading is used. This information is mentioned in the course outline.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the grading policy for the university?\"\n",
    "answer = full_chain.invoke({\"question\": question})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biggest mammal is the blue whale. It can grow up to 100 feet (30 meters) long and weigh as much as 200 tons (181 metric tonnes).\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the biggest mammal?\"\n",
    "answer = full_chain.invoke({\"question\": question})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Experiment with Retrieval Parameters <span style=\"color:green\">**[20 marks]**</span>\n",
    "\n",
    "In this task, you will experiment with different parameter values to analyze their impact on model efficiency. Focus on **understanding how** changes in these parameters affect the results, not just aiming for the best accuracy.\n",
    "\n",
    "#### Parameters to Experiment With:\n",
    "- **Chunk Size**: `200, 300, 400, 600`\n",
    "- **Chunk Overlap**: `100, 200`\n",
    "- **Top K**: `1, 2, 3`\n",
    "- **Thresholds**: `0.3, 0.5, 0.7`\n",
    "\n",
    "Feel free to explore additional values within similar ranges, but keep consistent gaps between values. Test **at least 3 values** for each parameter (5 for better results).\n",
    "\n",
    "#### Deliverables:\n",
    "- **Report**: Summarize your findings in a report, including:\n",
    "  - **Tables** for results\n",
    "  - **Graphs** for visual representation\n",
    "  - **Explanations** of trends observed and why they occur.\n",
    "  \n",
    "  *The report should explain why the chosen ranges make sense and provide insights into your findings.*\n",
    "\n",
    "- **Code**: Include the code you used to generate the graphs below each corresponding cell for reproducibility.\n",
    "\n",
    "#### Evaluation Criteria:\n",
    "- **Ranges and Explanations**: Marks are based on the **quality of your chosen ranges** and the **clarity of your explanations**. \n",
    "  - For example, simply choosing the highest chunk size and achieving 1 accuracy in all cases is not the correct approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "1. Previously, you worked with documents extracted from a PDF. In this section, you will use a **CSV file** as the data source. \n",
    "2. Since the Pinecone free tier allows a maximum of **5 indexes**, we need a function to automatically delete the **least-used index** when this limit is reached.\n",
    "3. Also some helper functions for logging and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the modified prompt template\n",
    "template = \"\"\"\n",
    "You are a chatbot that can only provide answers based on the following provided context. \n",
    "Only use the context below to answer the question.\n",
    "If the context doesn't provide any information to answer the question, say \"I don't know\".\n",
    "Provide a brief, one-word answer if possible.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "# Function to split context into chunks with overlap\n",
    "def split_text_into_chunks(text, chunk_size=500, overlap=50):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunk = text[i:i + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# Function to add documents to Pinecone\n",
    "def add_text_to_pinecone(texts, vector_store):\n",
    "    uuids = [str(uuid4()) for _ in range(len(texts))]\n",
    "    vector_store.add_texts(texts=texts, ids=uuids)\n",
    "\n",
    "# Function to delete index (you can do manually from Pinecone website as well) (you can change it to not delete handbook index)\n",
    "def delete_least_used_index(pc):\n",
    "    indexes = pc.list_indexes().names()\n",
    "\n",
    "    if len(indexes) >= 5:\n",
    "        index_to_delete = indexes[-1]\n",
    "        pc.delete_index(index_to_delete)\n",
    "        print(f\"Deleted least-used index: {index_to_delete}\")\n",
    "\n",
    "# Function to evaluate a question's response\n",
    "def evaluate_answer(predicted_answer, true_answer):    \n",
    "    # Compare the predicted answer with the true answer (case insensitive, and strip spaces)\n",
    "    predicted_answer = predicted_answer.strip().lower()\n",
    "    true_answer = true_answer.strip().lower()\n",
    "\n",
    "    return predicted_answer == true_answer\n",
    "\n",
    "# Function to log conversations and results to a text file\n",
    "def log_conversation_to_file(filename, chunk_size, overlap, top_k, question, context, predicted_answer, true_answer):\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(f\"Chunk Size: {chunk_size}, Overlap: {overlap}, Top K: {top_k}\\n\")\n",
    "        f.write(f\"Context: {context}\\n\")\n",
    "        f.write(f\"Question: {question}\\n\")\n",
    "        f.write(f\"Predicted Answer: {predicted_answer}\\n\")\n",
    "        f.write(f\"True Answer: {true_answer}\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step: Prepare Vector Store for a Dataset\n",
    "\n",
    "For each `(chunk_size, overlap)` pair, you need to:\n",
    "1. Initialize Pinecone and create/reuse an index.  \n",
    "2. Split dataset contexts into chunks.  \n",
    "3. Upload all chunks to the vector DB.  \n",
    "4. Call your `delete_least_used_index()` if needed.  \n",
    "\n",
    "This function ensures you only upload chunks **once per chunking configuration**, saving time during experiments.\n",
    "\n",
    "<span style=\"color:red\">!!! <span style=\"color:white\">why not take top k and threshold into consideration at this point? (add answer to your report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_vector_store(dataset, chunk_size=500, overlap=50):\n",
    "    # Initialize Pinecone\n",
    "    pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "    index_name = f\"test-{chunk_size}-{overlap}\"\n",
    "    \n",
    "    # Create/reuse vector store\n",
    "    try:\n",
    "        vector_store = initialize_pinecone(pc, index_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Pinecone: {e}, returning\")\n",
    "        delete_least_used_index(pc)\n",
    "        try:\n",
    "            vector_store = initialize_pinecone(pc, index_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Second attempt failed: {e}, exiting\")\n",
    "            return None\n",
    "        \n",
    "\n",
    "    all_chunks = []\n",
    "    for _, row in dataset.iterrows():\n",
    "        context = row[\"context\"]\n",
    "        chunks = split_text_into_chunks(context, chunk_size=chunk_size, overlap=overlap)\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "    add_text_to_pinecone(all_chunks, vector_store)\n",
    "\n",
    "    print(f\"Added {len(all_chunks)} chunks to {index_name}.\")\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step: Test a Single Question\n",
    "\n",
    "We define a helper function to run a single question through the RAG chain.\n",
    "\n",
    "#### Requirements:\n",
    "- Input: `question`, `chain`.  \n",
    "- Output: model’s predicted answer string.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_question(question, chain):\n",
    "    predicted_answer = chain.invoke(question)\n",
    "    return predicted_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step: Complete Run \n",
    "A helper function to run a complete test with the given parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_rag(dataset, chunk_sizes, overlaps, top_k_values, thresholds, test_prompt=prompt, llm=llm, exp_name=\"log\"):\n",
    "    results = []\n",
    "\n",
    "    for chunk_size in chunk_sizes:\n",
    "        for overlap in overlaps:\n",
    "            vector_store = prepare_vector_store(dataset, chunk_size, overlap)\n",
    "\n",
    "            for top_k in top_k_values:\n",
    "                for threshold in thresholds:\n",
    "                    retriever = create_retriever(vector_store, top_k=top_k, score_threshold=threshold)\n",
    "                    test_chain = build_rag_chain(retriever, test_prompt, llm)\n",
    "                    filename = exp_name + f\"_cs{chunk_size}_ov{overlap}_k{top_k}_th{threshold}.txt\"\n",
    "\n",
    "                    correct, total = 0, 0\n",
    "                    for _, row in dataset.iterrows():\n",
    "                        question = row[\"question\"]\n",
    "                        context = row[\"context\"]\n",
    "                        true_answer_dict = eval(row[\"answers\"])\n",
    "                        true_answer = true_answer_dict['text'][0] if true_answer_dict['text'] else \"I don't know\"\n",
    "\n",
    "                        predicted = test_question(question, test_chain)\n",
    "\n",
    "                        log_conversation_to_file(filename, chunk_size, overlap, top_k, question, context, predicted, true_answer)\n",
    "                        if evaluate_answer(predicted, true_answer):\n",
    "                            correct += 1\n",
    "                        total += 1\n",
    "\n",
    "                    accuracy = correct / total if total > 0 else 0\n",
    "                    results.append({\n",
    "                        \"chunk_size\": chunk_size,\n",
    "                        \"overlap\": overlap,\n",
    "                        \"top_k\": top_k,\n",
    "                        \"threshold\": threshold,\n",
    "                        \"accuracy\": accuracy\n",
    "                    })\n",
    "\n",
    "                    print(f\"cs={chunk_size}, ov={overlap}, k={top_k}, th={threshold} → Acc={accuracy:.4f}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Run Experiments on Dataset Splits\n",
    "\n",
    "We will now use the `experiment_rag()` function to test different parameter combinations.  \n",
    "\n",
    "#### Requirements:\n",
    "1. Load the dataset from CSV.  \n",
    "2. Split it into two halves:  \n",
    "   - **d1** → first 10 rows (e.g., unanswerable questions).  \n",
    "   - **d2** → last 10 rows (e.g., answerable questions).  \n",
    "3. Run `experiment_rag()` separately on each split.  \n",
    "4. Compare the results for **d1** vs **d2**.  \n",
    "\n",
    "> 💡 *Hint:* Choose a few values for `chunk_sizes`, `overlaps`, `top_k_values`, and `thresholds` so the experiments run in reasonable time.\n",
    "\n",
    "#### Primary Focus:\n",
    "- The **primary rows of interest** are the 10 rows in **d2** (answerable questions).  \n",
    "- Improvements seen in **d1** can likely be attributed to prompt engineering, which can be explored further in the bonus section.\n",
    "\n",
    "#### Reporting:\n",
    "- In your report, place more emphasis on the **d2 subset** of the dataset, as it is the key focus of this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Results on d1 (first 10 rows) ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GNG\\AppData\\Local\\Temp\\ipykernel_12480\\1127639835.py:20: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()  # Use CPU for embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 79 chunks to test-200-50.\n",
      "cs=200, ov=50, k=2, th=0.5 → Acc=0.4000\n",
      "cs=200, ov=50, k=4, th=0.5 → Acc=0.4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GNG\\AppData\\Local\\Temp\\ipykernel_12480\\1127639835.py:20: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()  # Use CPU for embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 228 chunks to test-200-150.\n",
      "cs=200, ov=150, k=2, th=0.5 → Acc=0.3000\n",
      "cs=200, ov=150, k=4, th=0.5 → Acc=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GNG\\AppData\\Local\\Temp\\ipykernel_12480\\1127639835.py:20: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()  # Use CPU for embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 24 chunks to test-600-50.\n",
      "cs=600, ov=50, k=2, th=0.5 → Acc=0.3000\n",
      "cs=600, ov=50, k=4, th=0.5 → Acc=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GNG\\AppData\\Local\\Temp\\ipykernel_12480\\1127639835.py:20: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()  # Use CPU for embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 30 chunks to test-600-150.\n",
      "cs=600, ov=150, k=2, th=0.5 → Acc=0.4000\n",
      "cs=600, ov=150, k=4, th=0.5 → Acc=0.4000\n",
      "\n",
      "### Results on d2 (last 10 rows) ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GNG\\AppData\\Local\\Temp\\ipykernel_12480\\1127639835.py:20: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()  # Use CPU for embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 67 chunks to test-200-50.\n",
      "cs=200, ov=50, k=2, th=0.5 → Acc=0.9000\n",
      "cs=200, ov=50, k=4, th=0.5 → Acc=0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GNG\\AppData\\Local\\Temp\\ipykernel_12480\\1127639835.py:20: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()  # Use CPU for embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 193 chunks to test-200-150.\n",
      "cs=200, ov=150, k=2, th=0.5 → Acc=0.8000\n",
      "cs=200, ov=150, k=4, th=0.5 → Acc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GNG\\AppData\\Local\\Temp\\ipykernel_12480\\1127639835.py:20: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()  # Use CPU for embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 23 chunks to test-600-50.\n",
      "cs=600, ov=50, k=2, th=0.5 → Acc=1.0000\n",
      "cs=600, ov=50, k=4, th=0.5 → Acc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GNG\\AppData\\Local\\Temp\\ipykernel_12480\\1127639835.py:20: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()  # Use CPU for embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 25 chunks to test-600-150.\n",
      "cs=600, ov=150, k=2, th=0.5 → Acc=1.0000\n",
      "cs=600, ov=150, k=4, th=0.5 → Acc=1.0000\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load dataset\n",
    "dataset = load_dataset(\"test_subset.csv\")\n",
    "\n",
    "# Step 2: Split dataset into two parts\n",
    "d1 = dataset.head(10)  # First 10 rows\n",
    "d2 = dataset.tail(10)  # Last 10 rows\n",
    "\n",
    "# Step 3: Define parameter values to test\n",
    "chunk_sizes = [200, 600]          # TODO: Adjust as needed\n",
    "overlaps = [50, 150]              # TODO: Adjust as needed\n",
    "top_k_values = [2, 4]             # TODO: Adjust as needed\n",
    "thresholds = [0.5]      # TODO: Adjust as needed\n",
    "\n",
    "# Step 4: Run experiments on each subset\n",
    "print(\"### Results on d1 (first 10 rows) ###\")\n",
    "results_d1 = experiment_rag(d1, chunk_sizes, overlaps, top_k_values, thresholds, exp_name=\"d1\")\n",
    "\n",
    "print(\"\\n### Results on d2 (last 10 rows) ###\")\n",
    "results_d2 = experiment_rag(d2, chunk_sizes, overlaps, top_k_values, thresholds, exp_name=\"d2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Part: Prompt Engineering <span style=\"color:green\">**[10 marks]**</span> \n",
    "\n",
    "\n",
    "In this part, your task is to **enhance the prompt** used in the previous experiments by applying prompt engineering techniques to improve the performance.\n",
    "\n",
    "#### Requirements:\n",
    "1. **Improve the Prompt**: Apply prompt engineering techniques to create a more effective and generalized prompt.  \n",
    "2. **Report Addition**: Extend your report from the previous part to include:\n",
    "   - How you came up with the improved prompt.\n",
    "   - Why you believe the new prompt performs better than the provided prompt.\n",
    "   - A comparison of the results (accuracy) between the original and improved prompt, focusing on **d2** (unanswerable questions).\n",
    "\n",
    "#### Key Objectives:\n",
    "- **Performance Tuning**: The goal is **not** to achieve the best accuracy but to **optimize performance** through prompt engineering.\n",
    "- **Generalization**: Ensure that the improved prompt works for both **answerable (d1)** and **unanswerable (d2)** questions, maintaining a balance in performance across both subsets. Your prompt should not only excel in **d2** but also maintain good performance in **d1**.\n",
    "\n",
    "In your report, provide an explanation of the trade-offs made during prompt engineering and how it influences the results in both subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_template = \"\"\"Respond to the following question:\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "improved_prompt = PromptTemplate(template=improved_template, input_variables=[\"question\"])\n",
    "\n",
    "# improved_chain = build_rag_chain(retriever, improved_prompt, llm) #(for manual testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### Results on d1 (first 10 rows) ###\")\n",
    "results_d1 = experiment_rag(d1, chunk_sizes, overlaps, top_k_values, thresholds, test_prompt=improved_prompt, exp_name=\"bonus\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Part 1\n",
    "\n",
    "You must submit:  \n",
    "- The **current notebook file** (`.ipynb`).  \n",
    "- Its **Python conversion** (`.py` file).  \n",
    "- The **Report** (`.pdf`).\n",
    "- Run **files** (`.txt`).\n",
    "\n",
    "All files should be placed inside a folder named \"RollNumber_PA1\". This folder must also include your **Part 2 files**, and the entire folder should be **zipped and submitted**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
