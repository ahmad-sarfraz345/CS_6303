{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Part 1 Instructions **[50 marks]**\n",
    "\n",
    "## Overview\n",
    "You are required to develop a **Retrieval-Augmented Generation (RAG) system** from scratch. This includes implementing embeddings, storing them in a vector database, building a retriever, and chaining it with an LLM. You will then **experiment with retrieval parameters** and analyze their effects on system accuracy. A bonus section will allow you to refine the system with **prompt engineering**.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Build the RAG Chain (20 marks)\n",
    "\n",
    "### Requirements\n",
    "- Implement embeddings and store them in a **vector database**.  \n",
    "- Implement a retriever and build the **RAG chain** that connects it with an LLM.  \n",
    "- Ensure the pipeline can retrieve relevant chunks and generate answers.  \n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: Chaining for Extended Context (10 marks)\n",
    "\n",
    "### Requirements\n",
    "- Implement **chaining** that allows the LLM to go beyond retrieved context.  \n",
    "- Demonstrate how chaining enables multi-step reasoning or broader context handling.  \n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: Parameter Testing & Report (20 marks)\n",
    "\n",
    "### Requirements\n",
    "- Use the provided dataset of **20 questions** (10 with one-word answers, 10 with no answer).  \n",
    "- Explore different values for:  \n",
    "  - **Chunk size**  \n",
    "  - **Chunk overlap**  \n",
    "  - **Top-K retrieval**  \n",
    "  - **Similarity cutoff threshold**  \n",
    "- Record and analyze how parameter choices affect accuracy.  \n",
    "- Submit a **report** summarizing:  \n",
    "  - The results of your experiments.  \n",
    "  - Explanations of why certain parameters improved or degraded performance.  \n",
    "\n",
    "---\n",
    "\n",
    "## Bonus: Prompt Engineering (5+5 marks)\n",
    "\n",
    "### Requirements\n",
    "- Refine your system using **prompt engineering** so that:  \n",
    "  - For answerable questions → the LLM provides the correct answer.  \n",
    "  - For unanswerable questions → the LLM responds with *“I don’t know”*.  \n",
    "\n",
    "### Notes:\n",
    "- Bonus marks can be used to recover deductions from earlier parts.  \n",
    "- Creativity and clarity in prompt design will be rewarded.  \n",
    "- Do **not** use GPT or other AI tools to generate code. Instead, consult the documentation — learning to use these tools independently is essential for your course projects.  \n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Criteria\n",
    "- **Correctness**: The RAG chain should work end-to-end.  \n",
    "- **Implementation**: Proper embeddings, vector DB, retriever, and chaining.  \n",
    "- **Analysis**: Depth of reasoning in your parameter exploration report.  \n",
    "- **Bonus (Optional)**: Effective prompt engineering to balance correctness and abstention.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "First, we install the required packages:\n",
    "\n",
    "- **LangChain** and **LangChain Community**: provide loaders, splitters, retrievers, etc.  \n",
    "- **LangChain HuggingFace**: for embedding models.  \n",
    "- **LangChain Pinecone** and **pinecone-client**: for vector database storage.  \n",
    "- **python-dotenv**: for managing API keys securely.  \n",
    "- **Streamlit**: (optional) for building a user interface.  \n",
    "- **MistralAI client**: for the language model we’ll use.\n",
    "\n",
    "This ensures our environment has all dependencies for building a RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-community in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (0.3.30)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain) (0.3.76)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain) (0.4.31)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain) (2.11.9)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-community) (3.12.15)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-community) (2.11.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=2.1.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-community) (2.3.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain-huggingface in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: pypdf in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (6.1.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.70 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-huggingface) (0.3.76)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-huggingface) (0.22.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.33.4 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-huggingface) (0.35.1)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.31)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (6.0.3)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (24.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.11.9)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.0.0)\n",
      "Requirement already satisfied: filelock in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.9.0)\n",
      "Requirement already satisfied: requests in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.67.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.25.0)\n",
      "Requirement already satisfied: anyio in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (4.11.0)\n",
      "Requirement already satisfied: certifi in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (2.5.0)\n",
      "Requirement already satisfied: colorama in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.33.4->langchain-huggingface) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain-pinecone in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (0.2.12)\n",
      "Requirement already satisfied: pinecone-client in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (6.0.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-pinecone) (0.3.76)\n",
      "Requirement already satisfied: pinecone<8.0.0,>=6.0.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (7.3.0)\n",
      "Requirement already satisfied: numpy>=1.26.4 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-pinecone) (2.3.3)\n",
      "Requirement already satisfied: langchain-openai>=0.3.11 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-pinecone) (0.3.33)\n",
      "Requirement already satisfied: httpx>=0.28.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-pinecone) (0.28.1)\n",
      "Requirement already satisfied: simsimd>=5.9.11 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-pinecone) (6.5.3)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (0.4.31)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (6.0.3)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (24.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (2.11.9)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (3.0.0)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2025.8.3)\n",
      "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.8.0)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.5.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.3 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.32.5)\n",
      "Requirement already satisfied: aiohttp>=3.9.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (3.12.15)\n",
      "Requirement already satisfied: aiohttp-retry<3.0.0,>=2.9.1 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (3.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.20.1)\n",
      "Requirement already satisfied: anyio in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from httpx>=0.28.0->langchain-pinecone) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from httpx>=0.28.0->langchain-pinecone) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.28.0->langchain-pinecone) (0.16.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.104.2 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-openai>=0.3.11->langchain-pinecone) (1.109.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langchain-openai>=0.3.11->langchain-pinecone) (0.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain-openai>=0.3.11->langchain-pinecone) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain-openai>=0.3.11->langchain-pinecone) (0.11.0)\n",
      "Requirement already satisfied: sniffio in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain-openai>=0.3.11->langchain-pinecone) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.104.2->langchain-openai>=0.3.11->langchain-pinecone) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (0.4.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai>=0.3.11->langchain-pinecone) (2025.9.18)\n",
      "Requirement already satisfied: orjson>=3.9.14 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (0.25.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from python-dateutil>=2.5.3->pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.17.0)\n",
      "Requirement already satisfied: colorama in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.104.2->langchain-openai>=0.3.11->langchain-pinecone) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: streamlit in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (1.50.0)\n",
      "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from streamlit) (6.2.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from streamlit) (8.3.0)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from streamlit) (2.3.3)\n",
      "Requirement already satisfied: packaging<26,>=20 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from streamlit) (2.3.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from streamlit) (11.3.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from streamlit) (6.32.1)\n",
      "Requirement already satisfied: pyarrow>=7.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from streamlit) (21.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from streamlit) (2.32.5)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from streamlit) (4.15.0)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from streamlit) (3.1.45)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from streamlit) (6.5.2)\n",
      "Requirement already satisfied: jinja2 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.5.0)\n",
      "Requirement already satisfied: colorama in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\coding\\cs_6303\\pa1\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain-community\n",
    "%pip install langchain-huggingface pypdf\n",
    "%pip install langchain-pinecone pinecone-client\n",
    "%pip install python-dotenv streamlit\n",
    "%pip install -qU langchain-mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\coding\\CS_6303\\PA1\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\coding\\CS_6303\\PA1\\.venv\\Lib\\site-packages\\langchain_pinecone\\__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from uuid import uuid4\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter as RCTS\n",
    "from langchain_mistralai import ChatMistralAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: RAG Chatbot  <span style=\"color:green\">**[20 marks]**</span>\n",
    "\n",
    "For guidance, you may refer to the following resources:  \n",
    "- [RAG Overview](https://python.langchain.com/docs/tutorials/rag/#overview)  \n",
    "- [How-to Guides](https://python.langchain.com/docs/how_to/)  \n",
    "- [Concepts](https://python.langchain.com/docs/concepts/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting API Keys\n",
    "\n",
    "We need API keys for external services:\n",
    "- [HuggingFace](https://huggingface.co/settings/tokens) (for embeddings)\n",
    "- [Pinecone](https://app.pinecone.io/) (for vector storage)\n",
    "- [MistralAI](https://console.mistral.ai/api-keys) (for the chat model)\n",
    "\n",
    "Here we define them as variables. Later we’ll store them securely using environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables are saved to .env file.\n"
     ]
    }
   ],
   "source": [
    "# You only need to run this cell once to create the .env file with your API keys.\n",
    "\n",
    "HUGGINGFACE_API_KEY = \"placeholder, .env created\"  # Replace with your Hugging Face API key\n",
    "PINECONE_API_KEY = \"placeholder, .env created\"        # Replace with your Pinecone API key\n",
    "MISTRALAI_API_KEY = \"placeholder, .env created\"       # Replace with your Mistral AI API key\n",
    "\n",
    "env_content = f\"\"\"\n",
    "HUGGINGFACE_API_KEY={HUGGINGFACE_API_KEY}\n",
    "PINECONE_API_KEY={PINECONE_API_KEY}\n",
    "MISTRALAI_API_KEY={MISTRALAI_API_KEY}\n",
    "\"\"\"\n",
    "\n",
    "with open(\".env\", \"w\") as file:\n",
    "    file.write(env_content)\n",
    "\n",
    "print(\"Environment variables are saved to .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Environment Variables\n",
    "\n",
    "We use `dotenv` to manage keys.  \n",
    "Instead of hardcoding secrets, we load them from a `.env` file.  \n",
    "This keeps our notebook secure and avoids accidentally sharing keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Splitting Documents\n",
    "\n",
    "Our knowledge source is a **PDF document** (the Undergraduate Student Handbook).  \n",
    "Steps:\n",
    "1. Load the PDF using `PyPDFLoader`.  \n",
    "2. Split the text into smaller chunks using `RecursiveCharacterTextSplitter`.  \n",
    "\n",
    "Why split?  \n",
    "- Large text doesn’t fit into the LLM’s context window.  \n",
    "- Smaller chunks make retrieval more accurate during question-answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Undergraduate Student Handbook 2021-2022.pdf'\n",
    "loader = PyPDFLoader(file_path)\n",
    "documents = loader.load()\n",
    "\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 4\n",
    "\n",
    "text_splitter = RCTS(chunk_size=chunk_size, chunk_overlap=chunk_overlap, add_start_index=True)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Initialize Pinecone and Create a Vector Store\n",
    "\n",
    "Your task is to **set up Pinecone** as the vector database for this RAG system.  \n",
    "\n",
    "#### Requirements:  \n",
    "- If the index with the given name already exists, reuse it. Otherwise, **create a new index**.  \n",
    "- Use the following configuration for the index:\n",
    "  - **Dimension:** 768  \n",
    "  - **Metric:** cosine similarity  \n",
    "  - **Cloud:** AWS, region `us-east-1`  \n",
    "- Initialize the **embedding model** (`HuggingFaceEmbeddings`).  \n",
    "- Return a **`PineconeVectorStore`** object.  \n",
    "\n",
    "> 💡 *Hint:* Make sure you return the vector store at the end of the function so it can be used later in your RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_pinecone(pc, index_name):\n",
    "    # TODO: Check if index_name already exists, otherwise create it\n",
    "    # - Use dimension=768\n",
    "    # - Metric=\"cosine\"\n",
    "    # - Cloud=\"aws\"\n",
    "    # - Region=\"us-east-1\"\n",
    "\n",
    "    existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "    if index_name not in existing_indexes:\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=768,\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "        )\n",
    "\n",
    "    # TODO: Initialize embeddings model (default)\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings()  # Use CPU for embeddings\n",
    "\n",
    "    # TODO: Load the index and initialize the PineconeVectorStore\n",
    "    index = pc.Index(index_name)\n",
    "    vector_store = PineconeVectorStore(index=index, embedding=embeddings) \n",
    "\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Add Documents to Pinecone\n",
    "\n",
    "Your task is to **add documents to the Pinecone vector store**.  \n",
    "\n",
    "#### Requirements:\n",
    "- Each document must be assigned a **unique ID** before insertion.  \n",
    "- Use `uuid4()` to generate IDs for all documents.  \n",
    "- Add the provided list of texts to the given **vector store**.  \n",
    "- Ensure the function returns nothing but successfully inserts the documents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_documents_to_pinecone(documents, vector_store):\n",
    "    # TODO: Generate a unique ID for each text using uuid4\n",
    "    uuids = [str(uuid4()) for _ in range(len(documents))]  \n",
    "\n",
    "    # TODO: Add texts and their IDs to the vector store\n",
    "    vector_store.add_texts(texts=[doc.page_content for doc in documents], ids=uuids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Create a Retriever from Vector Store\n",
    "\n",
    "Your task is to **create a retriever** that will query the vector store for the most relevant documents.  \n",
    "\n",
    "#### Requirements:\n",
    "- Convert the `vector_store` into a retriever using `.as_retriever()`.  \n",
    "- Use **similarity with score threshold** as the search type.  \n",
    "- The retriever should take two parameters:  \n",
    "  - **top_k** → maximum number of documents to retrieve.  \n",
    "  - **score_threshold** → minimum similarity score required for retrieval.  \n",
    "- Return the retriever object.  \n",
    "\n",
    "> 💡 *Hint:* Adjusting `top_k` and `score_threshold` will be important later when you experiment with retrieval performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever(vector_store, top_k=2, score_threshold=0.5):\n",
    "    # TODO: Create a retriever using similarity with score threshold\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": top_k, \"score_threshold\": score_threshold})  \n",
    "\n",
    "    return retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a Simple Similarity Search\n",
    "\n",
    "Now that you have implemented the helper functions, it’s time to test the full retrieval pipeline.\n",
    "\n",
    "#### Steps:\n",
    "1. Initialize Pinecone using your API key (`PINECONE_API_KEY`) and then a vector store.  \n",
    "2. Add handbook documents to the vector store using `add_documents_to_pinecone()`.  \n",
    "3. Run a **similarity search** against the vector store with a query (e.g., *\"Grading Policy\"*).  \n",
    "4. Retrieve the top `k` chunks and print out their content and metadata.  \n",
    "\n",
    "> 💡 *Hint:* This step ensures that your embeddings and vector store are working correctly before moving on to building the RAG chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GNG\\AppData\\Local\\Temp\\ipykernel_9192\\1127639835.py:20: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings()  # Use CPU for embeddings\n",
      "C:\\Users\\GNG\\AppData\\Local\\Temp\\ipykernel_9192\\1127639835.py:20: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()  # Use CPU for embeddings\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize Pinecone and create/reuse vector store\n",
    "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "vector_store = initialize_pinecone(pc, index_name=\"handbook\")\n",
    "\n",
    "# Step 2: we will use the previously created `docs` variable\n",
    "add_documents_to_pinecone(docs, vector_store)\n",
    "\n",
    "# Step 3: Initialize retriever\n",
    "k = 2\n",
    "threshold = 0.5\n",
    "retriever = create_retriever(vector_store, top_k=k, score_threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GNG\\AppData\\Local\\Temp\\ipykernel_9192\\679859536.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = retriever.get_relevant_documents(\"Grading Policy\")\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Perform a similarity search with a query\n",
    "results = retriever.get_relevant_documents(\"Grading Policy\")\n",
    "\n",
    "# Step 5: Print out the content and metadata of the retrieved results\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Chat Model (Mistral AI)\n",
    "\n",
    "We now set up the **LLM** that will generate answers.  \n",
    "Here we initialize the **[Mistral model](https://docs.mistral.ai/getting-started/models/models_overview/)** using our API key.  \n",
    "\n",
    "This model will take the retrieved handbook text + user question, and generate a clear answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = \"ministral-3b-latest\"\n",
    "llm = ChatMistralAI(\n",
    "    model=model,\n",
    "    temperature=0,\n",
    "    max_retries=1,\n",
    "    api_key=os.getenv(\"MISTRALAI_API_KEY\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Prompt Template\n",
    "\n",
    "Language models work best when guided with **clear instructions**.  \n",
    "Here we create a **PromptTemplate** that tells the chatbot exactly how to behave:\n",
    "\n",
    "- It is designed to answer questions **only about the LUMS Student Handbook**.  \n",
    "- It must use the **given context** (retrieved chunks from the handbook).  \n",
    "- If the answer is not in the context, it should respond with **“I don’t know”**.  \n",
    "- The template takes two inputs:  \n",
    "  1. **context** – text retrieved from the handbook.  \n",
    "  2. **question** – the student’s query.  \n",
    "\n",
    "This ensures the chatbot gives reliable answers and avoids making things up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the modified prompt template\n",
    "template = \"\"\"\n",
    "You are a chatbot designed to answer questions from LUMS students. LUMS is a university and you have access to the student handbook.\n",
    "Use following extract from the handbook to answer the question.\n",
    "If the context doesn't contain any relevant information to the question, then just say \"I don't know\".\n",
    "If you don't know the answer, then just say \"I don't know\".\n",
    "Do NOT make something up.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RAG Chain\n",
    "\n",
    "Now that you have a retriever & LLM, it’s time to connect both with a **prompt template** to form the RAG pipeline.\n",
    "\n",
    "### Requirements:\n",
    "- Take in three inputs:\n",
    "  - **retriever** → to fetch relevant context from the vector store.  \n",
    "  - **prompt** → the prompt template that structures inputs for the LLM.  \n",
    "  - **llm** → the language model used for generation.  \n",
    "- Chain the components together in the following order:\n",
    "  1. Retrieve documents (`retriever | format_docs`).  \n",
    "  2. Pass context and the user question into the **prompt**.  \n",
    "  3. Send the formatted prompt to the **LLM**.  \n",
    "  4. Parse the response into a clean string (`StrOutputParser`).  \n",
    "- Return the completed RAG chain object.  \n",
    "\n",
    "> 💡 *Hint:* This chain will be the foundation for answering user queries with retrieved context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):  # stitches together retrieved documents\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "def build_rag_chain(retriever, prompt, llm):\n",
    "    # TODO: Combine retriever, prompt, and llm into a chain\n",
    "    rag_chain = (\n",
    "        # 1. Retrieve docs and format them\n",
    "        {\"context\": retriever | format_docs,\n",
    "         \"question\": RunnablePassthrough()} # using runnable pass through to pass the question as is (from the .invoke)\n",
    "        # 2. Fill prompt with context + question\n",
    "        | prompt\n",
    "        # 3. Send to LLM\n",
    "        | llm\n",
    "        # 4. Parse clean string output\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return rag_chain\n",
    "\n",
    "rag_chain = build_rag_chain(retriever, prompt, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Test the RAG Chain\n",
    "\n",
    "Now that your RAG chain is built, it’s time to **run a query** and inspect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grading policy for LUMS is outlined in the Student Handbook. According to the handbook, grades are assigned on a scale of 0 to 100, with the following breakdown:\n",
      "\n",
      "- A: 90-100\n",
      "- B: 80-89\n",
      "- C: 70-79\n",
      "- D: 60-69\n",
      "- F: Below 60\n",
      "\n",
      "Additionally, the handbook specifies that a grade of 50 or above is considered passing.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the grading policy for the university?\"\n",
    "result = rag_chain.invoke(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "question = \"What are some minors offered at SBASSE?\"\n",
    "result = rag_chain.invoke(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Testing its limitations\n",
    "\n",
    "General questions that are not related to the university would not answered due to the way we structured the prompt template. Try asking such questions below to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "question = \"What are some gift ideas for Mothers Day?\"\n",
    "result = rag_chain.invoke(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the fastest growing plant?\"\n",
    "result = rag_chain.invoke(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Dynamic Routing <span style=\"color:green\">**[10 marks]**</span>\n",
    "\n",
    "One of the key advantages of LangChain is the ability to create **non-deterministic chains**, where the output of one step determines the next.  \n",
    "\n",
    "In this section, you will:\n",
    "1. **Build a classifier chain** to decide whether a user question is about *education/academic policies* or *Other*. [5 marks]  \n",
    "2. **Build a general LLM chain** that handles non-policy queries. [5 marks]  \n",
    "3. **Implement routing logic** that uses the classifier output to decide whether to send the query to the **RAG chain** (for policy-related questions) or to the **general chain** (for everything else). [10 marks]  \n",
    "\n",
    "#### Requirements:\n",
    "- Use a **prompt template** for classification (must only return `\"education/academic policies\"` or `\"Other\"`).  \n",
    "- Build a **general chain** that simply responds directly to a user’s query.  \n",
    "- Implement a `route()` function that:  \n",
    "  - Sends questions about education/academic policies → `rag_chain`.  \n",
    "  - Sends all other questions → `general_chain`.  \n",
    "- Combine everything into a `full_chain` that takes a question and automatically routes it.  \n",
    "\n",
    "> 💡 *Hint:* Use `RunnableLambda` for your routing logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example prompts for classification and general QA\n",
    "\n",
    "classifier_template = \"\"\"Given the user question below, classify it as either being about `education/academic policies`, or `Other`.\n",
    "\n",
    "Do not respond with anything other than 'education/academic policies' or 'Other'.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Classification:\"\"\"\n",
    "\n",
    "general_template = \"\"\"Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the classifier chain\n",
    "classifier_prompt = PromptTemplate(template=classifier_template, input_variables=[\"question\"])\n",
    "classifier_chain = {\"question\": RunnablePassthrough()} | classifier_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "# 2. Define the general LLM chain\n",
    "general_prompt = PromptTemplate(template=general_template, input_variables=[\"question\"])\n",
    "general_chain = {\"question\": RunnablePassthrough()} | general_prompt | llm | StrOutputParser()\n",
    "\n",
    "# 3. Implement routing logic \n",
    "def route(info):\n",
    "    # TODO: Implement routing logic based on the topic\n",
    "    label = info[\"pred\"].strip().lower()\n",
    "    question = info[\"question\"][\"question\"]\n",
    "    if label == \"education/academic policies\":\n",
    "        return rag_chain.invoke(question)\n",
    "    else:\n",
    "        return general_chain.invoke({\"question\": question})\n",
    "\n",
    "# Combine into the full chain\n",
    "full_chain = {\"pred\": classifier_chain, \"question\": RunnablePassthrough()} | RunnableLambda(route)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Our Improved ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grading policy for LUMS is outlined in the Student Handbook. According to the handbook, grades are assigned on a scale of 0 to 100, with the following breakdown:\n",
      "\n",
      "- A: 90-100\n",
      "- B: 80-89\n",
      "- C: 70-79\n",
      "- D: 60-69\n",
      "- F: Below 60\n",
      "\n",
      "Additionally, the handbook specifies that a grade of 50 or above is considered passing.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the grading policy for the university?\"\n",
    "answer = full_chain.invoke({\"question\": question})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biggest mammal is the blue whale. It can grow up to 100 feet (30 meters) long and weigh as much as 200 tons (181 metric tonnes).\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the biggest mammal?\"\n",
    "answer = full_chain.invoke({\"question\": question})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Experiment with Retrieval Parameters <span style=\"color:green\">**[20 marks]**</span>\n",
    "\n",
    "In this task, you will experiment with different parameter values to analyze their impact on model efficiency. Focus on **understanding how** changes in these parameters affect the results, not just aiming for the best accuracy.\n",
    "\n",
    "#### Parameters to Experiment With:\n",
    "- **Chunk Size**: `200, 300, 400, 600`\n",
    "- **Chunk Overlap**: `100, 200`\n",
    "- **Top K**: `1, 2, 3`\n",
    "- **Thresholds**: `0.3, 0.5, 0.7`\n",
    "\n",
    "Feel free to explore additional values within similar ranges, but keep consistent gaps between values. Test **at least 3 values** for each parameter (5 for better results).\n",
    "\n",
    "#### Deliverables:\n",
    "- **Report**: Summarize your findings in a report, including:\n",
    "  - **Tables** for results\n",
    "  - **Graphs** for visual representation\n",
    "  - **Explanations** of trends observed and why they occur.\n",
    "  \n",
    "  *The report should explain why the chosen ranges make sense and provide insights into your findings.*\n",
    "\n",
    "- **Code**: Include the code you used to generate the graphs below each corresponding cell for reproducibility.\n",
    "\n",
    "#### Evaluation Criteria:\n",
    "- **Ranges and Explanations**: Marks are based on the **quality of your chosen ranges** and the **clarity of your explanations**. \n",
    "  - For example, simply choosing the highest chunk size and achieving 1 accuracy in all cases is not the correct approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "1. Previously, you worked with documents extracted from a PDF. In this section, you will use a **CSV file** as the data source. \n",
    "2. Since the Pinecone free tier allows a maximum of **5 indexes**, we need a function to automatically delete the **least-used index** when this limit is reached.\n",
    "3. Also some helper functions for logging and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the modified prompt template\n",
    "template = \"\"\"\n",
    "You are a chatbot that can only provide answers based on the following provided context. \n",
    "Only use the context below to answer the question.\n",
    "If the context doesn't provide any information to answer the question, say \"I don't know\".\n",
    "Provide a brief, one-word answer if possible.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "# Function to split context into chunks with overlap\n",
    "def split_text_into_chunks(text, chunk_size=500, overlap=50):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunk = text[i:i + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# Function to add documents to Pinecone\n",
    "def add_text_to_pinecone(texts, vector_store):\n",
    "    uuids = [str(uuid4()) for _ in range(len(texts))]\n",
    "    vector_store.add_texts(texts=texts, ids=uuids)\n",
    "\n",
    "# Function to delete index (you can do manually from Pinecone website as well) (you can change it to not delete handbook index)\n",
    "def delete_least_used_index(pc):\n",
    "    indexes = pc.list_indexes().names()\n",
    "\n",
    "    if len(indexes) >= 5:\n",
    "        index_to_delete = indexes[-1]\n",
    "        pc.delete_index(index_to_delete)\n",
    "        print(f\"Deleted least-used index: {index_to_delete}\")\n",
    "\n",
    "# Function to evaluate a question's response\n",
    "def evaluate_answer(predicted_answer, true_answer):    \n",
    "    # Compare the predicted answer with the true answer (case insensitive, and strip spaces)\n",
    "    predicted_answer = predicted_answer.strip().lower()\n",
    "    true_answer = true_answer.strip().lower()\n",
    "\n",
    "    return predicted_answer == true_answer\n",
    "\n",
    "# Function to log conversations and results to a text file\n",
    "def log_conversation_to_file(filename, chunk_size, overlap, top_k, question, context, predicted_answer, true_answer):\n",
    "    with open(filename, 'a', encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Chunk Size: {chunk_size}, Overlap: {overlap}, Top K: {top_k}\\n\")\n",
    "        f.write(f\"Context: {context}\\n\")\n",
    "        f.write(f\"Question: {question}\\n\")\n",
    "        f.write(f\"Predicted Answer: {predicted_answer}\\n\")\n",
    "        f.write(f\"True Answer: {true_answer}\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step: Prepare Vector Store for a Dataset\n",
    "\n",
    "For each `(chunk_size, overlap)` pair, you need to:\n",
    "1. Initialize Pinecone and create/reuse an index.  \n",
    "2. Split dataset contexts into chunks.  \n",
    "3. Upload all chunks to the vector DB.  \n",
    "4. Call your `delete_least_used_index()` if needed.  \n",
    "\n",
    "This function ensures you only upload chunks **once per chunking configuration**, saving time during experiments.\n",
    "\n",
    "<span style=\"color:red\">!!! <span style=\"color:white\">why not take top k and threshold into consideration at this point? (add answer to your report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_vector_store(dataset, chunk_size=500, overlap=50):\n",
    "    # Initialize Pinecone\n",
    "    pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "    index_name = f\"test-{chunk_size}-{overlap}\"\n",
    "    \n",
    "    # Create/reuse vector store\n",
    "    try:\n",
    "        vector_store = initialize_pinecone(pc, index_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Pinecone: {e}, returning\")\n",
    "        delete_least_used_index(pc)\n",
    "        try:\n",
    "            vector_store = initialize_pinecone(pc, index_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Second attempt failed: {e}, exiting\")\n",
    "            return None\n",
    "        \n",
    "\n",
    "    all_chunks = []\n",
    "    for _, row in dataset.iterrows():\n",
    "        context = row[\"context\"]\n",
    "        chunks = split_text_into_chunks(context, chunk_size=chunk_size, overlap=overlap)\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "    add_text_to_pinecone(all_chunks, vector_store)\n",
    "\n",
    "    print(f\"Added {len(all_chunks)} chunks to {index_name}.\")\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step: Test a Single Question\n",
    "\n",
    "We define a helper function to run a single question through the RAG chain.\n",
    "\n",
    "#### Requirements:\n",
    "- Input: `question`, `chain`.  \n",
    "- Output: model’s predicted answer string.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_question(question, chain):\n",
    "    predicted_answer = chain.invoke(question)\n",
    "    return predicted_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step: Complete Run \n",
    "A helper function to run a complete test with the given parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_rag(dataset, chunk_sizes, overlaps, top_k_values, thresholds, test_prompt=prompt, llm=llm, exp_name=\"log\"):\n",
    "    results = []\n",
    "\n",
    "    for chunk_size in chunk_sizes:\n",
    "        for overlap in overlaps:\n",
    "            vector_store = prepare_vector_store(dataset, chunk_size, overlap)\n",
    "\n",
    "            for top_k in top_k_values:\n",
    "                for threshold in thresholds:\n",
    "                    retriever = create_retriever(vector_store, top_k=top_k, score_threshold=threshold)\n",
    "                    test_chain = build_rag_chain(retriever, test_prompt, llm)\n",
    "                    filename = exp_name + f\"_cs{chunk_size}_ov{overlap}_k{top_k}_th{threshold}.txt\"\n",
    "\n",
    "                    correct, total = 0, 0\n",
    "                    for _, row in dataset.iterrows():\n",
    "                        question = row[\"question\"]\n",
    "                        context = row[\"context\"]\n",
    "                        true_answer_dict = eval(row[\"answers\"])\n",
    "                        true_answer = true_answer_dict['text'][0] if true_answer_dict['text'] else \"I don't know\"\n",
    "\n",
    "                        predicted = test_question(question, test_chain)\n",
    "\n",
    "                        log_conversation_to_file(filename, chunk_size, overlap, top_k, question, context, predicted, true_answer)\n",
    "                        if evaluate_answer(predicted, true_answer):\n",
    "                            correct += 1\n",
    "                        total += 1\n",
    "\n",
    "                    accuracy = correct / total if total > 0 else 0\n",
    "                    results.append({\n",
    "                        \"chunk_size\": chunk_size,\n",
    "                        \"overlap\": overlap,\n",
    "                        \"top_k\": top_k,\n",
    "                        \"threshold\": threshold,\n",
    "                        \"accuracy\": accuracy\n",
    "                    })\n",
    "\n",
    "                    print(f\"cs={chunk_size}, ov={overlap}, k={top_k}, th={threshold} → Acc={accuracy:.4f}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Run Experiments on Dataset Splits\n",
    "\n",
    "We will now use the `experiment_rag()` function to test different parameter combinations.  \n",
    "\n",
    "#### Requirements:\n",
    "1. Load the dataset from CSV.  \n",
    "2. Split it into two halves:  \n",
    "   - **d1** → first 10 rows (e.g., unanswerable questions).  \n",
    "   - **d2** → last 10 rows (e.g., answerable questions).  \n",
    "3. Run `experiment_rag()` separately on each split.  \n",
    "4. Compare the results for **d1** vs **d2**.  \n",
    "\n",
    "> 💡 *Hint:* Choose a few values for `chunk_sizes`, `overlaps`, `top_k_values`, and `thresholds` so the experiments run in reasonable time.\n",
    "\n",
    "#### Primary Focus:\n",
    "- The **primary rows of interest** are the 10 rows in **d2** (answerable questions).  \n",
    "- Improvements seen in **d1** can likely be attributed to prompt engineering, which can be explored further in the bonus section.\n",
    "\n",
    "#### Reporting:\n",
    "- In your report, place more emphasis on the **d2 subset** of the dataset, as it is the key focus of this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Results on d1 (first 10 rows) ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GNG\\AppData\\Local\\Temp\\ipykernel_9192\\1127639835.py:20: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()  # Use CPU for embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 79 chunks to test-200-50.\n",
      "cs=200, ov=50, k=3, th=0.3 → Acc=0.4000\n",
      "cs=200, ov=50, k=3, th=0.5 → Acc=0.4000\n",
      "cs=200, ov=50, k=3, th=0.7 → Acc=0.4000\n",
      "\n",
      "### Results on d2 (last 10 rows) ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GNG\\AppData\\Local\\Temp\\ipykernel_9192\\1127639835.py:20: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()  # Use CPU for embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 67 chunks to test-200-50.\n",
      "cs=200, ov=50, k=3, th=0.3 → Acc=0.7000\n",
      "cs=200, ov=50, k=3, th=0.5 → Acc=0.7000\n",
      "cs=200, ov=50, k=3, th=0.7 → Acc=0.7000\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Step 1: Load dataset\n",
    "dataset = load_dataset(\"test_subset.csv\")\n",
    "\n",
    "# Step 2: Split dataset into two parts\n",
    "d1 = dataset.head(10)  # First 10 rows\n",
    "d2 = dataset.tail(10)  # Last 10 rows\n",
    "\n",
    "# Step 3: Define parameter values to test\n",
    "chunk_sizes = [200]\n",
    "overlaps = [50]\n",
    "top_k_values = [3]\n",
    "thresholds = [0.3, 0.5, 0.7]\n",
    "\n",
    "# Step 4: Run experiments on each subset\n",
    "print(\"### Results on d1 (first 10 rows) ###\")\n",
    "results_d1 = experiment_rag(d1, chunk_sizes, overlaps, top_k_values, thresholds, exp_name=\"d1\")\n",
    "\n",
    "time.sleep(60)  # wait for a while to avoid rate limiting\n",
    "\n",
    "print(\"\\n### Results on d2 (last 10 rows) ###\")\n",
    "results_d2 = experiment_rag(d2, chunk_sizes, overlaps, top_k_values, thresholds, exp_name=\"d2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Part: Prompt Engineering <span style=\"color:green\">**[10 marks]**</span> \n",
    "\n",
    "\n",
    "In this part, your task is to **enhance the prompt** used in the previous experiments by applying prompt engineering techniques to improve the performance.\n",
    "\n",
    "#### Requirements:\n",
    "1. **Improve the Prompt**: Apply prompt engineering techniques to create a more effective and generalized prompt.  \n",
    "2. **Report Addition**: Extend your report from the previous part to include:\n",
    "   - How you came up with the improved prompt.\n",
    "   - Why you believe the new prompt performs better than the provided prompt.\n",
    "   - A comparison of the results (accuracy) between the original and improved prompt, focusing on **d2** (unanswerable questions).\n",
    "\n",
    "#### Key Objectives:\n",
    "- **Performance Tuning**: The goal is **not** to achieve the best accuracy but to **optimize performance** through prompt engineering.\n",
    "- **Generalization**: Ensure that the improved prompt works for both **answerable (d1)** and **unanswerable (d2)** questions, maintaining a balance in performance across both subsets. Your prompt should not only excel in **d2** but also maintain good performance in **d1**.\n",
    "\n",
    "In your report, provide an explanation of the trade-offs made during prompt engineering and how it influences the results in both subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_template = \"\"\"Respond to the following question:\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "improved_prompt = PromptTemplate(template=improved_template, input_variables=[\"question\"])\n",
    "\n",
    "# improved_chain = build_rag_chain(retriever, improved_prompt, llm) #(for manual testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### Results on d1 (first 10 rows) ###\")\n",
    "results_d1 = experiment_rag(d1, chunk_sizes, overlaps, top_k_values, thresholds, test_prompt=improved_prompt, exp_name=\"bonus\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Part 1\n",
    "\n",
    "You must submit:  \n",
    "- The **current notebook file** (`.ipynb`).  \n",
    "- Its **Python conversion** (`.py` file).  \n",
    "- The **Report** (`.pdf`).\n",
    "- Run **files** (`.txt`).\n",
    "\n",
    "All files should be placed inside a folder named \"RollNumber_PA1\". This folder must also include your **Part 2 files**, and the entire folder should be **zipped and submitted**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
